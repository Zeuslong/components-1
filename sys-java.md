# Java 基础

## Object

### == & equals()

`==` 测试的两个引用是否指向了同一个对象。

`equals()` 如果被重写，则按照重写规则匹配；如果 `equals()` 没有重写，那么默认为 Object 当中的 `equals()` ，其中的实现是使用 `==`。

基本数据类型：byte、short、char、int、float、long、double、boolean 中 `==` 比较的是他们的值是否相等。

引用数据类型使用 `== `比较的时候比较的是他们的堆内存地址。

String、Integer、Date 在这些类当中 `equals()` 有其自身的实现（一般都是用来比较对象的成员变量值是否相同），而不再是比较类在堆内存中的存放地址了。 

### hashCode()

在 Object 类中，`hashCode()`方法是本地方法，具体计算 hash 的值根据 JVM 的实现有不同，也可以通过 `	--hashCode = 1` 来指定使用特定的 hash 计算方式。

在没有重写的 `equals()` 当中，采用 `==` 来比较对象，所以如果两个对象相同，那么它们的 `hashCode()`值一定要相同；又因为 hash 计算本身可能会产生冲突，所以如果两个对象的 `hashCode()` 相同，它们并不一定相同。

### wait()

`wait()` 方法能够让持有锁的一个线程让出锁，然后进入锁对象的等待池，如果再次被唤醒，就进入锁对象的锁池。

当获取锁失败，或者自己放弃锁后，线程会被加入到一个 Wait set 当中去，等待被唤醒，也就是等待别的线程释放锁的时候，会唤醒这 wait set 当中的某一个或全部线程。（notify / notifyAll）

### clone()

Object.clone 方法会返回一个不同地址的新对象，但是其中的引用还是原来对象当中的引用。

属于浅克隆。

补充：

- 浅克隆：被复制对象的所有变量都含有与原来的对象相同的值，而所有的对其他对象的引用仍然指向原来的对象。也就是说，浅复制仅仅复制所考虑的对象，而不复制它所引用的对象。
- 深克隆：被复制对象的所有变量都含有与原来的对象相同的值，那些引用其他对象的变量将指向被复制过的新对象，而不再是原有的那些被引用的对象。也就是说，深复制把要复制的对象所引用的对象都复制了一遍。

## String

### 实现

String 底层采用的是 `byte[]` 数组作为基础数据结构，`byte[]` 数组通过 `final` 关键字修饰，所以是不可变的，可以在程序间共享。

由 coder 保存编码的方式，同时会缓存它的 hash 值。

### charAt()

由于采用了 `byte[]` 作为数据结构，所以在 `charAt()` 这种方法里面，都是先把 index 翻倍，然后在取相邻的两个 byte，一个 char 占两个 byte 的条件下，再把这个两个 byte 组合成一个 char 返回。

### equals() 

1. 使用 == 比较是否指向同一个对象；
2. 如果不是，判断传入的对象是不是 String 类型；
3. 如果是，则比较两者的编码方式是不是一样；
4. 如果同种编码方式，再调用对应的编码方式，挨个比较里面的值；

### intern()

`intern()` 是一个 native 方法。

它的作用是，如果常量池当中有一个包含了等同于此 String 对象的常量，那么就返回这个常量的引用，否则就在常量池当中创建这对象然后再返回这个对象的常量。

### 内存分配问题

所有通过 new 关键字创建的对象都在 **堆** 上面，而所有用 `""` 引用的对象都在常量池当中。

```
String index = new String("index");
/**
创建了两个对象：
"index" 在字符串常量池当中，如果字符常量里面没有 index 这个字面量，那么就会重新分配在字符常量池当中分配，如果有就算了
在堆当中创建了一个 String 的对象，然后其中的值引用了字符串常量池当中的字面量
**/
```

### StringBuilder 与 StringBuffer

- String 不可变；

- StringBuffer 内部使用 synchronized 是线程安全的，但是性能较低；

- StringBuilder 不是线程安全的，但是性能高；

## 集合

### HashMap

#### 什么是 HashMap

HashMap 是一个 **key-value** 映射形式的数据结构，它利用的 **数组** + **链表** + **树** 三种数据结构实现的。

#### get() 方法

get() 方法是通过用元素的 `hash 码`与 `table 的长度 - 1` 做**除余运算**然后找到桶位置，然后再加入到桶头节点或者桶上的链表/树当中去的。

1. 检查 table 的合法性，如果不合法返回 `null`；
2. 计算桶位置；
3. 如果当前桶位置的 key 和传入的参数 key 一样，那么就直接返回头节点；
4. 否则检查这个节点是不是 `TreeNode`，如果为 `TreeNode` 那么就返回 `getTreeNode()` 结果；
5. 否则遍历普通 `Node` ，如果找到一个 key 值与参数 key 相同的就返回，如果直到最后没有找到就返回 null；

#### put() 方法

1. 检查 table 是否为空，或者 table 的长度是否为 0 ，如果是的话，就执行 resize 方法进行 table 初始化；
2. 计算 hash 值找到桶位置；
3. 看当前桶位置是否为 **空**，如果为空就依据传进来的 key-value 创建 node 节点；
4. 如果不为空，就执行解决 hash 冲突的策略；
5. 用 `==` 与 `equals` 检查当前桶位置的 key 与传入的 key 是否相同，如果相同，那么就替换；
6. 检查该节点是否为 TreeNode 节点，如果是就执行 `putTreeVal()` 方法；
7. 如果是普通 Node 节点，依次查询这个链表上面的所有元素，查找是否有 key 值与传入的参数 key 相同的节点；
8. 如果在节点少找到元素，就替换；
9. 如果没有，就在末尾根据传入的参数新建节点；
10. 如果是替换操作，在替换完成的时候就返回 oldValue，如果是新增操作 `modCount + 1`，并且检查负载因子大小是否需要执行 `resize()`；

#### resize() 方法

`resize()` 方法最核心的地方在于要把原来链表上的元素重新散列到新的桶位置当中去，这个过程不是通过重新计算 hash 值的余数来完成的，而是通过 `(hash & oldCap) == 0` 来判断。`oldCap = 16`，加入桶的位置为 `hash % (oldCap - 1)`

1. 查看 table 是否有初始化过，然后重新设定容量与阈值大小；
2. 如果有初始化过；
  3. 检查是否大于或者等于最大的限定 `1<<30`，如果如果是，则把阈值设定为最大常数；
  4. 如果不是，那下一次扩容如果没有益处，并且旧的容量大与等于默认的初始容量，如果是右移一个单位(乘以2)；
5. 如果没有初始化过检查是否设定了负载因子；
6. 如果都没有初始化table并且没有设定阈值，那么重新计算阈值与设定容量大小；
7. 最后检查一次阈值，如果阈值还是为0，再次计算一次阈值；
8. 设定完容量大小与阈值后，开始重新设定 table；
9. 设定阈值与初始化 table 数组；
10. 遍历旧的 table 数组，取出元素并设定旧的 table 数组的元素位置为空；
11. 如果是一个 Node 节点，重新通过 hash 值计算位置；
12. 如果是 TreeNode 调用 `split()`；
13. 如果是 Node 链表，遍历 table 中的头元素，重新散列冲突的元素；
14. 用元素的 hash 值与原来的容量做比较，如果为 0 则表示以前的哈希值的容量位置对应的 1 的地方也为 0，那么用新的来散列的话还是原来的位置，就记录到新的链表 loHead 当中；
15. 如果做 & 运算的结果为 1 ，那么就表示新的容量 length - 1 来计算桶位置的时候，它的高位会影响它的新位置，那么就记录到 hiHead 当中；
16. 把 loHead 链表赋值给原来桶的位置，把 hiHead 赋值给新的桶位置(原来的桶位置+原来的容量)；

*散列冲突寻找新的桶位置的原理如下*：

```
// 原理

/**

假设原本的容量为 16
对应的二进制为 0001 0000

新的容量为 32
对应的二进制为 0010 0000

对象的 hash = 0001 0101

那么在原有的桶位置为

0000 1111
0001 0101
---------
0000 0101 = 5

也就只有后4位起了作用

在新的桶位置的位置为

0001 1111
0001 0101
---------
0001 0101 = 21 = 5 + 16

便是后5位起了作用

而原本的桶容量的二进制刚好为五位，就可以验证 hash 码的第五位是否为 1 ，如果为 1 ，就需要改变对象在新桶当中的位置

**/
```

#### Hash 计算桶位置

hash 一般用来计算桶的位置，通过 `hash % table.length - 1` 的方式得到桶位置

```
    static final int hash(Object key) {
        int h;
        return (key == null) ? 0 ： (h = key.hashCode()) ^ (h >>> 16);
    }
```

如果 key == null，那么 hash 默认为 0

如果不是，通过虚拟机为该 key 计算得到的一个 hashCode 值的前 16 位高位与后 16 位高位做异或运算然后得到一个 hash 值，这样做是为了混合原始 hash 值的高位和低位增加了低位的随机性。

#### HashMap 如何比较 Key

先求出 key 的`hashCode()`，比较其值是否相等，若相等再比较`equals()`，若 `equals()`相等则认为他们是相等的；若`equals()`不相等则认为他们不相等。

#### 当 Key 为自定义类时

当 Key 为自定义类时，需要重写 Key 的 `equals()` 方法与 `hashCode()` 方法。

HashMap 中的比较 key 是这样的：先求出 key 的`hashCode()`，比较其值是否相等，若相等再比较`equals()`，若 `equals()`相等则认为他们是相等的；若`equals()`不相等则认为他们不相等。
 如果只重写 `hashcode() ` 不重写 `equals()` 方法，当比较 `equals()` 时只是看他们是否为同一对象（即进行内存地址的比较），所以必定要两个方法一起重写。

#### 如何让 HashMap 散列更均匀

当容量为 2 的幂次方的时候，由于做 hash 找 index 的过程是 length-1 所以，偶数 - 1 变成奇数，基数当中的二进制最后一位是 1 ，这样就可以充分利用到 hashcode 当中的最后一位，从而散列的均匀一点。

对应在 HashMap 当中，则是计算 Key 的桶位置时，使用的是 Key 的 `hashCode()` 与 `table.length - 1` 来计算桶位置。

#### 为什么 HashMap 中 table 长度要为 2 的幂次方

默认的初始容量为 `1<< 4 (16)`，当容量一定是 2^n 的时候，那么末尾一定是 1，这个时候 `h & (length - 1) == h % length`，而按位运算会很快

#### HashMap 的负载因子是多少

在 Java Doc 注释里面有写，通常，HashMap 的 `load factory` 为 **0.75** 便可以在 **时间** 和 **空间** 上有一个较好的平衡。

太低的  `load factory` 会导致 HashMap 频繁的执行 `resize()` 方法来重新分配 table 空间；太高的  `load factory` 可以降低空间消耗，但是会增大查找的花费，因为较高的值如果初识数量大于负载因子所限制的最大条目数，那么就不会发生 `rehash` 操作。

#### fast-fail 策略

如果一个迭代器被创建了，一旦在迭代器中进行迭代时， HashMap 发生了结构变化，那么迭代器就会抛出异常 `ConcurrentModificationException`

#### HashMap 的死循环

HashMap 的死循环出现在并发环境下执行 `resize` 方法，当并发加入 put 的时候，引起扩容操作，然后多线程导致 HashMap 的 Entry 链表形成环形数据结构，查找时候会引起死循环/

具体的过程如下：

当有一个线程刚进入 get 方法的时候执行 `Entry next = e.next()` 方法后被挂起，然后新线程进入，并执行了扩容的操作，如果新的扩容后的链表的位置发生了反转，那么这个时候环形就形成了。

假如有两个线程 P1、P2，以及链表 `a->b->null`：

1. P1先执行，执行完`Entry<K，V> next = e.next;`代码后发生阻塞，或者其他情况不再执行下去，此时 `e=a，next=b`；

2. 而P2已经执行完整段代码，于是当前的新链表 `newTable[i] 为 b->a->null`；

3. P1又继续执行`Entry<K，V> next = e.next;`之后的代码，则执行完`e=next;`后，`newTable[i]`为`a<=>b`，则造成回路，`while(e!=null)`一直死循环；

#### HashMap 与 HashTable 区别

1. HashMap 不是线程安全的，HashTable 是线程安全的；

2. HashMap 允许 null 为键或值，Hashtable 不允许；

3. HashTable 直接使用对象的 hashCode 做运算，HashMap 会把 hashCode 的前后 16 位进行与运算；

4. 另一个区别是 HashMap 的迭代器 (Iterator) 是 fail-fast 迭代器，而 Hashtable 的 enumerator 迭代器不是 fail-fast 的。所以当有其它线程改变了 HashMap 的结构（增加或者移除元素），在迭代过程中将会抛出ConcurrentModificationException

5. HashMap 的数组初始化默认容量为 16，Hashtable 默认为 11；

#### HashMap 与 TreeMap 的区别

1. HashMap 不保证顺序；TreeMap 能根据键来排序，默认是升序排序，Iterator 遍历 TreeMap 得到的有序的结果；

#### 让 HashMap 同步

使用 Collctions 封装 HashMap 为 ConcurrentHashMap

```java
Map m = Collections.synchronizeMap(hashMap);
```

### ConcurrentHashMap

#### ConcurrentHashMap 数据结构

- 1.8 之前

  ConcurrentHashMap 当中维护一个 segment 数组，将元素分作若干部分，segment 继承了 ReentrantLock，在每一个 segment 当中又包含一个 HashEntry 数组。每一次并发都是第一次 hash 获取 segement 锁，再次 hash 找到元素。所以默认的 segment 数量为 16，也就只支持 16 个线程并发。

- 1.8 之后

  ConcurrentHashMap 当中维护了一个 Node 数组 + 链表/红黑树，Node 当中有 final 的 key 与 volatile 的 value 与 next。并发的方式改成了 synchronized 关键字控制。

在 ConcurrentHashMap 当中使用 volatile 关键来保证读取到工作内存当中的 table 都是最新的，所以 volatile 只会保证读取的时候数据是准确的。

在 ConcurrentHashMap 当中，如果要保证写入的时候操作是线程安全的话，那么还是需要 synchronized 来做线程同步操作。

#### ConcurrentHashMap 如何保证线程安全

ConcurrentHashMap 当中保证线程的安全性主要是通过 volatie、CAS、Synchronized 三个措施来保障线程安全性的。

其中 volatile 主要用来保证**读**的线程安全性。

在 ConcurrentHashMap 当中，用 volattile 来修饰 Node 数组，这样就保证了如果有一个线程修改了 Node 数组当中的数据，那么就会使读线程当中的 Node 数组无效，然后读线程就会重新在主内存当中去加载 Node 数组的数据到线程当中。

对于**写**操作的线程安全性的措施主要是依靠 CAS 与 Synchronized 关键在实现的。

写操作主要涉及到如下几个步骤：

1. 获取 value 的桶位置（也是利用 hash 与 Node 数组长度除余来获得的）；
2. 获取 Node 结点（通过 Unsafe 类的 Volatile 语义的方法来获取的）；
3. 插入 Node 结点（CAS）；
4. 插入 Node 到链表当中去（加锁 Synchronized）；

其中，如果是插入 Node 节点时，由于插入结点这个操作不会对其它的数据有影响，所以就可以用 CAS 来替代锁住整个 Node 结点数组这个过程，所以在并发的时候如果有必要加锁，那么数组的其它 Node 结点同样是可以访问的，也就提高了并发量。

对于插入 Node 结点到链表或者树当中去这个操作，就需要获取锁来进行操作。

在 ConcurrentHashMap 的注释当中有这样一句话：ConcurrentHahsMap 的查询操作只会保证获取到线程上的某一个时间的状态，而不会获取到最新的状态，这也就是说 ConcurrentHashMap 本身是没有采用读写锁，而在做更新操作的时候，根据更新的位置的不同来判断所采用的更新方式，通常情况下不会对 Node 数组加锁，那么也就不会对大部分的读操作有锁定的影响，会提升性能。

通过上面的这个两个过程就能保证 ConcurrentHashMap 在写的时候的线程安全性。

#### put() 方法

ConcurrentHashMap 的更新操作与 HashMap 也大致相同，但其中采用了 CAS 更新方法、synchronized 关键字来同步线程。

ConcurrentHashMap 执行更新操作分作两种情况，一是当前插入的地方是一个空节点的，二是当前插入的地方已经存在一个元素。

如果插入的地方是一个空节点，那么利用 CAS 原理，把元素插入。

如果插入的地方已经存在一个元素，那么就要先获取这个元素的锁，获取锁之前，要先检查是否有其它线程在扩容，那么有，则会完成扩容操作，然后获取这个结点的锁，从头开始遍历这个结点，遇到 hash 相同的就替换，否则末尾插入。

最后检查是否需要扩容或者转成树。

#### get() 方法

ConcurrentHashMap 的查找过程与 HashMap 大致相同。但是在获取的时候有两个关键的地方，一是被 volatile 关键字修饰的 table，二是 tabAt 方法。

ConcurrentHashMap 的底层 table 是被 volatile 修饰的，也就是说，每一次要去使用 table 的时候，如果有别的线程修改了 table，那么当前线程的 table 缓存就会失效，然后去内存的当中重新获取 table 元素，保证了 table 是某一个时刻上面最新的。

然后计算桶位置，在 table 上面获取节点，这个时候 tabAt 方法调用了 Unsafe 类当中的 getObjectVolatile 方法，这个方法是拥有 volatile 语意的，保持原子操作的情况下，去内存当中根据对象地址和内存地址的偏移量直接获取对象，也保证了该对象在某一时刻的最新性。

然后获取到链表的头结点后，按照访问链表的方式来遍历链表，并且不需要加锁。

#### 为什么 ConcurrentHashMap 的参数都不能为空

ConcurrentHashmap 和 Hashtable 都是支持并发的，这样会有一个问题，当你通过 `get(k)`获取对应的 value 时，如果获取到的是 null 时，你无法判断，它是 `put（k，v）` 的时候 value 为 `null`，还是这个 key 从来没有做过映射。
HashMap是非并发的，可以通过 `contains(key)` 来做这个判断。而支持并发的 Map 在调用 `m.contains(key)和 `m.get(key)` ，由于读取未上锁，m 可能已经不同了。

#### ConcurrentHashMap 的特殊点

1. ConcurrentHashMap 是一个类似于 HashTable 的线程安全的 HashMap，它遵循了 HashTable 的功能规范，含有 HashTable 当中每个方法的方法版本，但是与 HashTable 同步的细节不相同；
2. 在访问 ConcurrentHashMap 的哈希表的时候，所有的操作都是要求是线程安全的，但是检索操作通常是不需要的，并且使用方法的所有参数都不能为空，也不允许存在空的 key 与 value；
3. 它的状态查询方法例如：isEmpty()、size 都是反应某一个时刻的状态，其创建的迭代器也只是某一时刻所包含的键值对，并且只能被一个线程持有，而且不会有 ConcurrentModificationException；
4. ConcurrentHashMap 的 table 初始化发生在第一次插入的操作时候。

### ArrayList

1. ArrayList 本质上是一个可改变大小的数组。当元素加入时，其大小将会动态地增长。

2. 内部的元素可以直接通过 `get()` 与 `set()` 方法进行访问，随机访问很快。

3. 删除非头尾元素慢，新增元素慢。由于需要预留一部分空间用于后续元素的插入所以也相对比较浪费空间，较适用于无频繁增删的情况。

4. ArrayList 非线程安全。
5. 一直在末尾添加元素是，ArrayList 效率比 LinkedList 效率更高，ArrayList 虽然可能需要扩容，但是它的平均插入应该是 O(1)；LinkedList 在插入链表末尾的时候，不需要扩容，但是它的插入末尾需要移动指针到末尾，所以它的效率是 O(n)。

### LinkedList

1. LinkedList 是一个链表，在添加和删除元素时具有比 ArrayList 更好的性能。
2. 但在 `get()` 与 `set()` 方面弱于 ArrayList。
3. 适用于：没有大规模的随机读取，有大量的增加/删除操作。随机访问很慢，增删操作很快，不耗费多余资源。
4. 允许null元素。
5. 非线程安全。

### Vector

1. Vector 类似于 ArrayList，底层是数组，但其是同步的，开销就比 ArrayList 要大。
2. Vector 和 ArrayList 在更多元素添加进来时会请求更大的空间。Vector 每次请求其大小的双倍空间，而 ArrayList 每次对 size 增长 50%。

## 并发

### 线程

#### 什么是进程

进程是计算机当中已运行程序的实体，当操作系统调度进程的时候会给它独立的资源例如内存空间 CPU 资源等等。

#### 什么是线程

线程是进程当中的一个任务的描述，一个进程当中含有多个线程，线程可以利用进程所拥有的资源。

#### 线程与进程的区别

1. 进程是操作系统分配资源的最小单位，而线程是独立运行和独立调度的基本单位。
2. 进程享有独立的内存单元，而多个线程共享进程的内存空间。
3. 二者均可并发执行，但是进程的创建和销毁需要系统对进程分配或销毁所以线程在并发过程中的创建、切换、销毁成本更低。

#### 线程的生命周期

- 新建状态：当一个线程被关键字 `new` 创建出来的时候，就处于新建状态，JVM 为其分配内存空间与初始化其成员变量
- 就绪状态：当一个线程执行 `start()` 方法后，就处于就绪状态，或者当一个线程正在被调度运行的时候，也是处于就绪状态，等待获取系统资源被执行
- 运行状态：当一个线程得到了系统资源后，开始执行 `run()` 方法当中的代码后
- 阻塞状态：线程主动的让出 cpu 资源，等待某条件满足的状态就叫阻塞状态 1.执行 `sleep` 方法主动让出 cpu 资源 2.等待 IO 完成 3.执行 `wait` 后等待被唤醒 4.等待获取锁权限
- 死亡状态：当一个线程执行完 `run` 方法后，或者执行 `interrupt` 方法强制中断线程

#### start() 与 run()

`run()` 方法当中封装的是多线程需要处理的任务逻辑代码，如果单独执行`run()` 方法那么线程没有成功启动而只是在本线程当中执行。

`start()` 方法是启动新创建的线程，而且 `start()` 内部调用了 `run()` 方法。

#### Runnable 与 Callable

Runnable 接口不返回任何结果。

Callable 则需要返回结果。

#### Runnable 与 Thread

采用 Thread 的时候， 只能继承一个类， 如果这个类需要继承其它类则不考虑 Thread。

如果使用 Runnable 实现接口能够更好的独立出逻辑块， 因为一个 Runnable 通常被理解为一个任务。

#### interrupted() 与 isInterrupted() 

两者都是调用 `isInterrupted(boolean ClearInterrupted)` 方法，来判断当前运行的线程是否被中断了。

interrupted 传递的参数为 `true` 也就是会重置 `interrupted state` 状态变量，也就是说如果第二次调用，这个期间当前线程只被中断过一次的话，就会返回 `false`。

isInterruptedd 传递的参数为 `false` 不会重置 `interrupted state` 状态变量。

*!* `interrupted state` 用来标识当前线程是否被中断了。

#### notify() 与 notifyAll() 

我的理解中，在 Java 的并发当中所有的对象都存在一个 `锁队列` 和 `等待队列`，当执行对象的 `wait()` 方法的时候，就会把执行的线程加入到等待队列当中，等待队列当中的线程不参与锁的竞争，`notify()` 方法是用来唤醒等待队列当中某一个线程进入到锁队列当中去参与锁竞争，而 `notifyAll()` 方法则是唤醒等待队列当中的所有的线程到锁队列当中参与锁的竞争。

#### wait() 和 sleep() 

wait 的含义是在本对象的等待队列上等待被其它唤醒， 不仅仅要放弃当前所获得的锁权限， 而且在没有被唤醒的时候不参与锁的竞争。

sleep 的含义是休眠该线程， 也就是阻塞的意思， 不会放弃当前线程的锁权限， 也不会放弃争夺锁的权限。

#### ThreadLocal

ThreadLocal 是一种用空间换取线程安全的做法，线程可以把原本在内存共享区域的数据拷贝到自己的线程内存上面，如果线程修改了这个数据不会影响到主内存当中原本的数据，只会修改自己线程线程空间的那份数据。

ThreadLocal 是一个类，要使用到 ThreadLocal 的类需要内部持有这个类，ThreadLocal 内部有 get 和 set 方法来满足大部分需求，TheadLocal 利用 ThreadLocalMap 来存取对象，Map 当中 key 为 ThreadLocal 自己，value 为值。

#### Java 线程调度算法

采用抢占式算法。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行。

Java 采用抢占式线程调度的方法，每个线程由操作系统来分配时间，线程的切换不由线程本身来决定，在这种调度方式下，线程的执行时间是系统可控的，不会导致由于一个线程使整个进程阻塞的问题。

但是 Java 当中可以通过 `Thread.yield()` 方法来主动让出执行时间。

Java 当中也可以设置优先度，在 Java 当中有 10 个级别的优先度分别来映射操作系统上的优先级，可能没有办法一一对应，所以并不是很可靠，所以最终还是要取决与操作系统。

### 锁

#### 独享锁与共享锁

独享锁是指该锁一次只能被一个线程所持有。

共享锁是指该锁可被多个线程所持有。

ReentrantLock、Synchronized 是独享锁

ReadWriteLock 其读锁是共享锁，其写锁是独享锁。

#### 公平锁与非公平锁

公平锁指的是有多个线程申请锁的时候，按照线程申请的先后顺序来获取锁。

非公平锁指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。

synchronized 是非公平锁。

ReentrantLock 可以通过构造方法指定是公平锁还是非公平锁。

##### 比较

事实上公平的锁机制往往没有非公平的效率高，因为公平的获取锁没有考虑到操作系统对线程的调度因素，这样造成 JVM 对于等待中的线程调度次序和操作系统对线程的调度之间的不匹配。此外，在锁的快速且重复的获取过程中，连续获取的概率是非常高的，而公平锁会压制这种情况，虽然公平性得以保障，但是响应比却下降了。

公平锁能够减少“饥饿”发生的概率，等待越久的请求越是能够得到优先满足。

#### 互斥锁

互斥锁独享锁具体的实现。

互斥锁在 Java 中的具体实现就是 ReentrantLock。

#### 读写锁

读写锁就是共享锁的具体实现。在写入的时候只允许一个线程执行读写，当没有写入操作时，可以允许多个读操作并发执行。

读写锁在 Java 中的具体实现就是 ReadWriteLock。

#### 乐观锁与悲观锁

乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。

悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。

悲观锁适合写操作非常多的场景。在Java中的使用，就是利用各种锁。

乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断的更新数据。乐观的认为，不加锁的并发操作是没有事情的。

乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。在Java中的使用，是无锁编程，常常采用的是 CAS 算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。

#### 重入锁

重入锁指的是一个线程能够对一个临界资源重复加锁。例如，是当一个线程获取到一个对象的锁并执行其中的方法时，它调用这个对象当中的另外的一个方法，可以不用再进入锁队列去抢占锁的控制权，可以直接调用另外的一个方法。

在 Java 当中，ReentrantLock、synchronized 是重入锁。

### CAS 算法

CAS 全称**比较与交换（Compare and Swap）**。

CAS 会先备份旧的数据，然后基于旧的数据进行修改数据，当数据修改完成后，比较备份的旧数据与当前内存当中的数据，如果相等，则证明共享数据没有被修改，替换成新值；如果不相等，说明共享数据已经被修改，放弃已经所做的操作，然后重新执行刚才的操作。

### AbstractQueuedSynchronized

AQS 提供阻塞和唤醒线程功能以及队列模型的简单框架，许多同步类实现都依赖于它，如常用的 ReentrantLock。

#### 基础结构

AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是单项列表来实现的，将暂时获取不到锁的线程加入到队列中。AQS是通过将每条请求共享资源的线程封装成一个队列当中的节点来实现锁的分配。

AQS 中还维护了一个名为 state 的字段，意为同步状态，是由Volatile修饰的，用于展示当前临界资源的获锁情况。子类可以通过修改State字段来实现多线程的独占模式或者共享模式。

### ReentrantLock

ReentrantLock 是一个重入锁，也可以通过构造方法来指定其是否是一个公平锁，但默认是一个非公平锁。

其内部是通过扩展 AQS(AbstractQueuedSynchronizer) 来实现的。

```java
Lock lock = new ReentranLock();
lock.lock();
try{
    //do something
}finally{
    lock.unlock();
}
```

#### 获取锁

获取锁的过程主要与两个东西相关：

1. AQS，阻塞的线程都放在这个队列当中；
2. state 状态码，表示进入线程的次数，每重入一次会 +1，每释放一次 -1；

ReentrantLock 的 lock 过程其实是调用了 AQS 的 acquire 方法。

在 AQS 的 acquire 方法当中，线程获取当前的锁，主要有两个过程：1.尝试获取锁过程。2.进入阻塞队列。

尝试获取锁的过程：

首先判断 AQS 的 state 是否等于 0，state 为 0 则表示锁没有人占有，接着，hasQueuedPredecessors 判断队列是否有排在前面的线程在等待锁，没有的话调用 compareAndSetState 使用 cas 的方式修改 state，最后线程获取锁成功，然后将线程记录为独占锁的线程。

若果前面有人占着锁，就会检查是否是当前线程，如果是当前线程就继续执行。

如果两个判断都没有执行通过，就加入等待队列。

加入队列的时候使用的是一个死循环，保证一定会插入到队列当中，在加入等待队列的时候，Node 对象的初始化，会自动的获取当前线程，并传入到 Node 当中持有。

如果没有获取到锁并且成功的加入到等待队列当中后，就会中断当前线程。

#### 释放锁

在 AQS 当中有一个 state 状态码，用来维护当前线程进入的次数，保证线程的重入时候要释放相同次数，也是其它线程获取锁的一个参考参数。

释放锁的时候，主要对比当前的线程与当前锁拥有者是否一致，然后在改变 AQS 当中的 state 状态码，表示有一个线程已经退出。

然后通过 AQS 获取队列当中的线程来临界资源的占用。

## NIO

### 阻塞/非阻塞与同步/异步

阻塞与非阻塞指的是等待数据的一种方式。阻塞指的是数据还没有就绪时，就等待数据，直到数据就绪后再继续执行；非阻塞在数据没有就绪时，会直接返回，继续执行当前方法。

同步与异步指的是查询数据是否就绪的方式。同步指的是主动轮询各个数据准备的状态；异步指的是数据就位后通知进程数据已经就位。

综上，阻塞与非阻塞，讨论的是进程想要获取结果的时候的一种状态(等或者不等)，而异步与同步讨论的是，消费者获取服务者任务执行状态的一种方式。

### 什么是 NIO

NIO 叫做 nonblocking IO，是一种同步非阻塞的 IO 方式。

在普通的 IO 当中，当应用程序需要进行 IO 的时候，它会去调用系统当中的方法，这个系统调用会导致应用程序的阻塞，然后等待操作系统将数据拷贝到内核当中，再从内核拷贝到用户进程当中，只有到数据全部准备好之后，应用程序才会被唤醒，再进行数据的处理。

而 Java 当中的 NIO，在执行系统调用后，系统会立即给应用程序返回，然后 Java NIO 应用程序就不需要再等待，但是它会每一隔一段时间再来获取系统当中 IO 的状态，如果数据没有准备好的话，就会返回一个 ERROR，当数据返回后应用程序不需要再等待，可以继续做其它的事情，然后不断的对系统进行轮询直到找到自己可以用的数据为止。

### Java NIO 模型

Java NIO 模型是基于反应器模式来设计的。

Channel 把自己注册到 Selector 上面去，然后告诉 Selector 这个 Channel 所关心的事情，然后应用程序就会接着做别的事情。

Selector 会反复的对数据进行轮询，当 Channel 所感兴趣的事情发生的时候，Selector 就会唤醒这个 Channel 然后执行操作。

### NIO 主要组件

#### Channel

Channel 叫做管道，向 Buffer 当中写入数据，或者读取数据

- FileChannel

文件的数据读写

- DatagramChannel

UDP 数据的读写

- SocketChannel

TCP 数据的读写

- ServerSocketChannel

允许我们监听TCP链接请求，每个请求会创建会一个SocketChannel

#### Buffer

缓存区，主要是暂时保存 Channel 写入的数据，也是让 Channel 读取数据的地方

利用Buffer读写数据，通常遵循四个步骤：

```java
    把数据写入buffer；
    调用flip；
    从Buffer中读取数据；
    调用buffer.clear()或者buffer.compact()
```

具体的实现：

- ByteBuffer
- MappedByteBuffer
- CharBuffer
- DoubleBuffer
- FloatBuffer
- IntBuffer
- LongBuffer
- ShortBuffer

#### Selector

注册对各种 I/O 事件的兴趣的地方，而且当那些事件发生时，Selector 就会告诉您所发生的事件。

### FileChannel

FileChannel 是 Java NIO 类库当中处理文件数据的一个抽象类，虽然是 NIO 类库当中的一员，但是 FileChannel 在读取与写入数据的时候，依旧是阻塞的，也就是说在 FileChannel 在读入数据的时候，还是要等到数据进入到操作系统内核内存，然后再从内核读入进程内存，然后才可以开始操作。

但是个人认为 FileChannel 的出现对于 Java IO 体系来讲并不是多余的。网上流传的 NIO 有很多种含义：1.Non-blocking IO。2.New IO。第一种含义意思是非阻塞的 IO，也就是当我们执行其中的某些方法的时候，当方法没有执行完毕的时候，原本是应该被阻塞的，但是在非阻塞的 IO 方式当中，就算没有执行完毕，也是可以直接返回的。第二种含义的意思是一种新的 IO 思维，NIO 所有的 IO 类后都有一个后缀叫做 Channel，Channel 译过来是通道的意思，这一种含义更贴近 FileChannel 的设计理念，FileChannel 是一个双向操作的 IO 流，传入对应的文件路径并且打开 FileChannel 之后既可以执行读操作也可以执行写操作，在这样的工作机制下，当我们需要对文件进行操作的时候，只需要打开对应文件的 FileChannel，再申请一个 Buffer 就可以直接操作，不用关心读取的时候的一些细节的控制。

# JVM

## 并发

### Sychronized

Synchronized 是 Java 内置的一个关键字，用来防止资源冲突，当任务要执行被 synchronized 关键字保护的代码片段的时候，它将检查片段的锁是否被别的线程持有，然后获取锁，执行代码，释放锁。

#### 原理

在 JVM 中所有的对象都自动含有单一的锁，也叫做监视器。当在某一个对象上面调用任意的 synchronized 方法的时候，这个对象会被锁定，这时候其它再来请求该对象其它的 synchronized 方法只有等到前一个方法调用完毕并释放了锁之后才能被调用。所以在同一个对象当中，所有的 synchronized 的方法共享同一个锁。

#### 实现机制

synchronized 方法的实现第一步在编译成字节码的时候就开始了，JVM 的字节码中被 synchronized 修饰的方法在的 Class 文件方法表当中 access_flag 字段当中的 synchronized 标记为 1 表示是一个同步的方法。

编译器会把 synchronized 块编译成 `monitorenter` 与 `monitorexit` 包裹的代码块，然后把方法翻译成普通的方法。

JVM 被要求保证当线程执行到 `monitorenter` 与 `monitorexit` 要成对使用，并且任何一个对象都必须关联一个 monitor，当这个对象的 monitor 被持有后，将处于锁定的状态，不允许其它的线程进入到 monitor 的代码块。

当线程执行 `monitorenter` 的时候，对应的 monitor 对象会被锁定，然后触发对象的 monitor record 列表来获取锁的相关信息，退出便释放锁，如果 monitor 被持有，获取失败，也会刷新 monitor record 列表来获取锁信息。

### Volatile

被 Volatile 修饰的成员变量具有两成含义

1. 保证了不同线程对这个变量的修改对于其它的线程来讲是立即可见的
2. 禁止指令重排序

```java
//线程1
boolean stop = false;
while(!stop){
    doSomething();
}

//线程2
stop = true;
```

这段代码有可能导致无法中断线程，当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程 2 转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。

用volatile修饰之后

第一：使用volatile关键字会强制将修改的值立即写入主存；

第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）；

第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。

那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，
然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。

那么线程1读取到的就是最新的正确的值。

#### Volatile 实现原理

如果对声明了 volatile 变量进行写操作时，JVM 会向处理器发送一条 Lock 前缀的指令，JVM 会对 LOCK 进行特殊处理。

#### Volatile 可见性实现

如果对声明了 volatile 变量进行写操作时，JVM 会向处理器发送一条 Lock 前缀的指令，将这个变量所在缓存行的数据写会到系统内存。 这一步确保了如果有其他线程对声明了 volatile 变量进行修改，则立即更新主内存中数据。

在多处理器环境下，为了保证各个处理器缓存一致，每个处理会通过嗅探在总线上传播的数据来检查 自己的缓存是否过期，当处理器发现自己缓存行对应的内存地址被修改了，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作时，
会强制重新从系统内存把数据读到处理器缓存里。 这一步确保了其他线程获得的声明了 volatile 变量都是从主内存中获取最新的。

#### Volatile 有序性实现

Lock 前缀指令实际上相当于一个内存屏障（也成内存栅栏），它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面。
即在执行到内存屏障这句指令时，在它前面的操作已经全部完成。

#### i++ 是原子操作吗

不是

i++ 要经历三个过程：

1. 读取 i 值
2. 计算 i 值
3. 写回 i 值

这三个过程都可能被打断

### Monitor

Monitor 是一个同步机制，也是一个对象。

所有的 Java 对象从被创建起就关联一个 Monitor 对象，所以每个对象都有一把锁，叫做内部锁或者 Monitor 锁。

#### Monitor Record

monitor record 是线程私有的

Owner：初始时为NULL表示当前没有任何线程拥有该 monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为 NULL；

EntryQ：关联一个系统互斥锁（semaphore），阻塞所有试图锁住 monitor record 失败的线程。

RcThis：表示 blocked 或 waiting 在该 monitor record 上的所有线程的个数。

Nest：用来实现重入锁的计数。

HashCode：保存从对象头拷贝过来的 HashCode 值（可能还包含 GC age）。

Candidate：用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。

Candidate 只有两种可能的值： 0 表示没有需要唤醒的线程， 1 表示要唤醒一个继任线程来竞争锁。

### 对象头

在对象头当中存放了两部分信息：Mark Word(标记字段)、Klass Pointer(类型指针)。

Mark word 用来存放对象运行时候的数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。

Klass Pointer 用来指向这个对象的 类 元数据信息，可以根据 Klass Pointer 来判断该对象属于哪个类。

#### 对象头大小

如果对象为普通对象，对象头一般占两个机器码，在 32 位虚拟机上，1 个机器码占 4 字节，也就是 32 bit。

如果对象为数组，对象头占三个机器码，因为 JVM 可以通过对象的元数据信息确定对象的大小，但是无法从数据的元数据信息确认数组的大小，所以多余的一块用来记录数组的大小。

对象头信息是对象自身数据之外的信息，所以为了考虑到空间，对象头当中的 Mark Word 一般被设计成为一个非固定的数据结构以便在极小的空间内存放更多的数据。

### 锁优化



## 内存模型

### 什么是内存模型

内存模型是针对并发一块来讲的，C 与 C++ 语言在并发上都是使用物理硬件和操作系统的内存模型，就会导致在一个平台上并发没有问题，但是在另外一个平台上就会有线程安全的问题，所以 Java 为了在程序在各种平台下都能达到一致的内存访问效果，定了一种 Java 内存模型来屏蔽掉各种硬件和操作系统的内存访问差异。

Java 内存模型包括两部分：

1. 主内存与工作内存的交互模型；
2. 主内存与工作内存的交互规则；

### 主内存和工作内存的交互模型

Java 内存模型规定了所有的变量都存储在主内存中。每条线程中还有自己的工作内存，线程的工作内存中保存了被该线程所使用到的变量（这些变量是从主内存中拷贝而来）。
线程对变量的所有操作（读取，赋值）都必须在工作内存中进行。不同线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。

### 主内存与工作内存的交互规则

内存交换原则分作两个小块，1.变量交互规则。2.volatile 的特殊规定。

### 内存模型的实现

#### 指令重排序

在执行程序时，为了提高性能，编译器和处理器会对指令做重排序。但是，JMM确保在不同的编译器和不同的处理器平台之上，通过插入特定类型的Memory Barrier来禁止特定类型的编译器重排序和处理器重排序，为上层提供一致的内存可见性保证。

1. 编译器优化重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序；
2. 指令级并行的重排序：如果不存l在数据依赖性，处理器可以改变语句对应机器指令的执行顺序；
3. 内存系统的重排序：处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行；

#### 数据依赖

如果两个操作访问同一个变量，其中一个为写操作，此时这两个操作之间存在数据依赖性。

编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序，即不会重排序。

#### as-if-serial

不管怎么重排序，单线程下的执行结果不能被改变，编译器、runtime 和处理器都必须遵守 as-if-serial 语义。

#### 内存屏障（Memory Barrier ）

通过内存屏障可以禁止特定类型处理器的重排序，从而让程序按我们预想的流程去执行。内存屏障，又称内存栅栏，是一个CPU指令

编译器和 CPU 能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条 Memory Barrier 会告诉编译器和 CPU：不管什么指令都不能和这条 Memory Barrier 指令重排序。

Memory Barrier 所做的另外一件事是强制刷出各种 CPU cache，如一个 Write-Barrier（写入屏障）将刷出所有在 Barrier 之前写入 cache 的数据，因此，任何 CPU 上的线程都能读取到这些数据的最新版本。

如果一个变量是 volatile 修饰的，JMM 会在写入这个字段之后插进一个 Write-Barrier 指令，并在读这个字段之前插入一个 Read-Barrier 指令。

这意味着，如果写入一个 volatile 变量，就可以保证，一个线程写入变量 a 后，任何线程访问该变量都会拿到最新值。

#### happens-before 原则

在内存模型当中中，如果一个操作的执行结果需要对另一个操作可见，那么这两个操作之间必须要存在 happens-before 关系，这个的两个操作既可以在同一个线程，
也可以在不同的两个线程中。

监视器锁规则：对一个锁的解锁操作，happens-before 于随后对这个锁的加锁操作。

传递性规则：如果 A happens-before B，且 B happens-before C，那么 A happens-before C。

### 可见性

可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。

当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。

而普通的共享变量不能保证可见性，普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。

通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。

```java
//线程1执行的代码
int i = 0;

i = 10;

//线程2执行的代码
j = i;

```

由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到工作内存中，然后赋值为10，那么在线程1的工作内存当中i的值变为10了，却没有立即写入到主存当中。

此时线程2执行 j = i，它会先去主存读取i的值并加载到线程2的工作内存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10.

### 有序性

即程序执行的顺序按照代码的先后顺序执行。

在 CPU 执行的时候，可能会把代码的顺序打乱，然后来提高执行的效率，叫做指令重排序，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。

在Java里面，可以通过volatile关键字来保证一定的“有序性”。另外可以通过synchronized和Lock来保证有序性，synchronized和Lock保证每个时刻是有一个线程执行同步代码，
相当于是让线程顺序执行同步代码，就保证了有序性。

### 原子性

即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。

在 Java 当中只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作

```Java
int x = 10;
int y = x;
x++;
x = x + 1;
```

咋一看，可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。

语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。

语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。

同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。

所以上面4个语句只有语句1的操作具备原子性。

## 内存区域

### JVM 内存区域划分

根据 《Java 虚拟机规范》的规定, 运行时数据区通常包括这几个部分: 

- 程序计数器（Program Counter Register）
- 方法区（Method Area）
- Java 栈（VM stack）
- 本地方法栈（Native Method Stack）
- Java 堆 （VM heap）

JVM 规范当中虽然规定了程序在执行期间运行时数据区应该包括几部分, 具体实现没有做规定。

### 程序计数器

程序计数器存储的是当前线程所要执行的下一条字节码指令的地址, Java 是多线程的, 所以程序计数器是线程独立的才能保证线程之间不会相互干扰

Jvm 规范当中规定, 如果线程执行的非 Navive 的方法, 则程序计数器当中保存的是需要执行的指令的地址, 如果线程执行的是 navive 方法, 那么程序计数器中的值是 undefined

由于程序计数器当中的存储的数据所占空间的大小不会随程序的改变而改变, 所以程序计数器没有设定 OutOfMemory Exception 异常

### 方法区

方法区当中存储已经被虚拟机加载的 类信息 常量 静态变量 即使编译器编译后的代码等数据

如果一个类被 ClassLoader 加载, 那么就会在方法区生成一个代表该类的 Class 对象(唯一一种不在堆上生成的对象), 该对象将作为程序访问方法区当中该类的信息的外部接口, 该类也是实现 **反射** 的基础

因为所有被加载到内存的 Class 信息应该被所线程都能获取到, 所以方法区是线程共享的

Java 7 之前, HotSpot 将 Gc 分代收集扩展到了方法区, 使用永久代来实现方法区, 主要针对常量池的回收和类的卸载

Java 7 之后, 逐渐将方法区从永久代移除, Java 7 已经将运行时常量池永久从永久代移除, 在 Java 堆当中开辟了一块内存区域存放运行时常量池

Java 8, 永久代被彻底移除, 将方法区放到了一个与堆不相连的区域, 叫做元空间

规定了一种异常

- OutOfMemoryError 当方法区无法满足内存分配的需求时

### Java 栈

 Java 栈就是我们常说的栈, 也是 Java 方法执行的内存模型

 栈当中存放的是栈帧, 一个栈帧对应的是一个被调用的方法, 其中包括 局部变量表(Local Variable) 操作数栈(Operand Stack) 指向当前方法所属类的运行时常量池的引用(Reference to runtime constant pool) 方法返回地址(Return Address) 和一些额外的信息

 每一个方法从调用直至执行完成的过程, 就对应一个栈帧在薯泥机中入栈到出栈的过程

 每一个线程执行的方法集不一样, 所以每一个线程都有自己的一个 Java 栈, 保持线程的独立性

 Java 栈当中规定了两个异常情况

 - StackOverFlowError 线程请求的栈深度大于虚拟机所允许的栈深度
 - OutOfMemoryError 虚拟机栈动态扩展时无法申请足够的内存

### 本地方法栈

本地方法栈与 Java 栈的作用相似, 但是本地方法栈是为本地方法提供支持的

HotSpot 虚拟机将本地方法栈和虚拟机栈合二为一

本地方法栈定义了两个异常情况

 - StackOverFlowError 线程请求的栈深度大于虚拟机所允许的栈深度
 - OutOfMemoryError 虚拟机栈动态扩展时无法申请足够的内存

### Java 堆

Java 堆用来存放对象和数组, 基本上所有的对象都存放在 Java 的堆空间上

Java 堆是线程共享的, 所以当多个线程持有指向同一个堆当中的对象的时候, 就会发生并发安全问题

Java 堆是 GC 主要关注的区域, 按照 GC 的角度来观察, Java 堆被分作 新生代和老年代

Java 堆被要求是逻辑上相连的, 但它可以不是物理上相连接的

规定了一种异常

- OutOfMemoryError 表示在堆上没有内存再完成实例的分配, 而且也无法扩展堆的内存大小

### 内存泄漏与内存溢出

内存泄露: 指在内存当中的对象无法再被 GC 回收而一直占用内存块。

内存溢出: 指程序执行过程当中, 无法申请到足够的内存空间的一种情况。

### static 的代码块

static 修饰的变量会进入到方法区当中

static 修饰的代码也应该要进入到内存当中，因为字节码文件只是静态的文件代码，需要加载到内存当中才能成为可动态运行的对象

### 工作内存的回收

工作内存主要是线程的内存块，里面主要包含有栈。

栈里面的空间，主要是栈帧在使用，其中包括局部变量表、操作数栈、动态链接等等信息，这些信息在弹出栈并且使用完后会被自动清理。

### 常量池

#### 静态常量池

静态常量池是 Class 文件当中的一部分。我理解为 Class 文件的资源仓库, 主要存储字面量(例如, 文本字符串 & 声明为 final 的常量值等)和符号引用(例如类和接口的全限定命 & 字段的名称和描述符 & 方法的名称和描述符)

#### 动态常量池

动态常量池是方法区的一部分。它主要在 Class 被加载进入内存时用来存放 Class 文件当中编译时期生成的各种字面量和符号引用。

#### String 实例与常量池

```
  String s1 = new String("abc");

```

在这里，至少创建了两个对象, 第一个对象在常量池当中, 为 "abc", 执行这条语句的时候, 需要去检查常量池当中是否有这样一个字面量, 如果没有就需要创建, 当确定字面量之后, 就用这个字面量来创建字符串实例。

```
    String index = "a";
```

这句话会产生两个东西, 第一个是 Javac 在编译的时候会把 "a" 和 index 的符号引用写入到 class 文件的常量池当中, 这个时候只能被看作是数据, 在程序运行的时候, index 与 "a" 会被读入内存当中, 这个时候就变成了 String 类的实例, 具有 String 的所有的方法。

## GC

### 什么是 GC

在 Java 中 GC 主要负责回收未使用到的对象，部分 GC 算法还负责整理内存碎片。

### GC 可达性分析

从一系列 GC ROOTs 的对象开始查找, 从这些节点向下搜索这些 GC ROOT 的引用链, 如果一个对象不在任何一条引用链上, 那么这个对象就是不可用的

*!* 可作为 GC ROOT 的对象有一下几种

- 虚拟机栈(栈帧中的本地变量表)中引用的对象
- 方法区中（也就是那些被加载到内存当中的 Class 对象）静态属性引用的对象
- 方法区当中（也就是那些呗加载到内训当中的 Class 对象）常量引用的对象
- 本地方法栈当中 JNI(一般所谓的 Native) 引用的对象

### GC 引用计数法

给对象添加一个引用计数器, 每当有一个地方引用它的时候, 计数器便加一, 引用失效的时候, 计数器便减一, 当计数器为 0 的时候, 这个对象就是不可再用的了

引用计数法没有办法很好的解决两个对象相互循环引用的问题, 也就是 A 引用 B, B 引用 A, 这样 AB 两个对象永久在内存当中不会被回收。

### GC 回收算法

#### 标记-清除法

标记清除算法分作两个阶段, 第一个标记阶段标记处所有不可用的对象, 在第二个阶段清除这些阶段

两个不足:

1. 标记和清除阶段的执行效率都不高
2. 清除阶段过后会产生大量的不连续的空间, 这样就可能导致空间的随便较多, 无法分配需要内存空间较大的对象

#### 标记-清除法

标记 - 整理算法同样分作两个阶段, 第一个阶段标记所有不可用的对象, 第二个阶段让所有存活的对象都向一端移动, 然后清除边界以外的所有内存

标记 - 整理算法主要针对老年代, 老年代的对象都是长期存活的对象, 所以如果用复制算法就需要额外的内存空间担保, 所以班老年代都不采用复制算法

#### 复制算法

复制算法把内存分作两部分, 每次分配都只使用其中的一部分, 当使用的部分快要用完的时候, 就把这部分当中所有可用的对象复制到另外一块内存空间上, 然后一次性清除块占满的哪一块内存空间

复制算法解决了 标记 - 清除算法效率不高的问题, 同时只回收半块内存空间, 所以不考虑内存碎片的问题

98% 的对象的周期都很短, 为了提高内存空间的使用, 一般来讲, 不是按 1:1 来划分空间, 而是把内存空间划分为一块较大的 Eden 和两块较小的 Survivor, 其中 Eden : Survivor = 8:1, 每一次只使用其中一块 Eden 与 Survivor, 当需要进行垃圾把 Eden 与 Survivor 当中还存活的对象复制到另外一块没有使用的 Survivor 内存空间上

对于 HotSpot 来讲, 因为 Survivor 的空间较小, 所以可能会出现整理后对象复制到 Survivor 上没有足够的内存空间, 所以这个时候就会有一个担保规则

*!* 空间担保规则: 在发生 Minor GC 前, 虚拟机会检查老年代的可用的连续空间是否大于新生代所有对象总空间, 如果大于, 那么这个 Minor GC 就是安全的, 便可以用老年代进行空间担保. 如果老年代空间不足, 那么就需要进行一次 Full GC.

### GC 的内存分代

虚拟机中的共划分为三个代：年轻代（Young Generation）、年老点（Old Generation）和持久代（Permanent Generation）。其中持久代主要存放的是Java类的类信息，

#### 年轻代

所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。年轻代分三个区。一个Eden区，
两个Survivor区(一般而言)。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，
此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。

#### 老年代

在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。

*!* 特殊情况：大对象会被直接分配到老年代

#### 持久代

用于存放静态文件，如今Java类、方法等。

### GC 两个过程

#### Minor GC

回收新生代垃圾

当 Eden 区的空间占满的时候，会触发 Minor GC。

MajorGC采用标记—清除算法：首先扫描一次所有老年代，标记出存活的对象，然后回收没有标记的对象。MajorGC的耗时比较长，因为要扫描再回收。MajorGC会产生内存碎片，为了减少内存损耗，我们一般需要进行合并或者标记出来方便下次直接分配。

执行 Minor GC 操作时，不会影响到永久代。从永久代到年轻代的引用被当成 GC roots，从年轻代到永久代的引用在标记阶段被直接忽略掉。

大部分 Eden 区中的对象都能被认为是垃圾，永远也不会被复制到 Survivor 区或者老年代空间。所以 Minor GC 导致的 Stop-the-World 所需要的时间会很短。

#### Major GC

回收老年代垃圾

### GC 收集器

垃圾收集器是 GC 算法的具体实现, 需要根据老年代与新生代的特征来具体设计, JVM 规范当中没有强制限定的要求

新生代垃圾收集器: **Serial** **ParNew** **Parallel Scavenge**

老年代垃圾收集器: **CMS** **Serial Old** **Parallel Old**

全能收集器: **G1**

#### G1

G1 面向服务端应用的垃圾回收器, 负责管理整个堆内存

G1 相对其它的收集器具有如下四个特点

1. 并行与并发 依旧能用多线程的优势处理垃圾
2. 分代收集 依旧把对象根据年龄分代
3. 空间整合 采用标记 - 整理的算法, 局部采用复制算法, 不会产生碎片
4. 可预测的停顿 可以让使用者规定在运行的 M 时间当中, 停顿时间最长为 N

G1 收集器的过程可以分成以下几个步骤

1. 初始标记 (Stop the world, 标记 GC ROOT 能直接关联到的对象)
2. 并发标记 (可以与用户线程并发, 从 GC ROOT 进行可达性分析)
3. 最终标记 (Stop the wolrd 但是是可以与用户线程并发的, 主要是修正之前的标记)
4. 筛选回收 (因为只有部分空间而且停顿时间很短所以建议 Stop the world 但是是可以并发的, 这个阶段对各个 Region 的价值进行排序, 根据设定来执行回收)

##### G1 内存化整为零的思路

把 Java 堆分作多个大小相等独立区域, 这个独立区域(Region), 把对象分配在这独立区域上面, 当要发生 GC 的时候, 就只在这些 Region 上面发生, 虚拟机根据内部维护的一个优先列表来决定, 这个优先列表记录了回收可以获得空间大小以及回收空间所需要的时间的经验值, 同时还维护了一个 Remembered Set 来记录当前的 Region 的引用关系。

#### 新生代 GC

##### Serial 收集器

Serial 收集器回收新生代内存对象, 是最古老的一种收集器, 它在 Jdk 1.3.1 之前是新生代唯一的新生代收集器

Serial 在新生代采用复制算法

Serial 收集器是单线程的, 也就是说 Serial 收集器执行的时候, 只能是一个 CPU 或者 线程来执行垃圾收集过程, 所以在 Serial 工作期间, 会发生 Stop the world, 也就是停止该程序的所有其它操作, 也就是程序无响应

Serial 的优点在于, 由于没有线程交互的开销, 所以相对于其它的垃圾收集器简单而高效, 在单个 CPU 的环境可以获得最高的单线程收集效率

Serial 适用于 Client 客户端的开发

可以配合 CMS & Serial Old 老年代垃圾收集器一起工作

##### ParNew 收集器

ParNew 负责回收新生代内存中的对象, 采用复制算法

ParNew 是 Serial 收集器的多线程版本, 除了并行(指的是多个垃圾回收线程并行执行, 不过同样要暂停用户线程)的特性外, 其余都一致

在单 CPU 的条件下, 回收效率没有 Serial 高, 但是在并发情况下效果更好

ParNew 用于 Server 端, 是唯一款可以和 CMS 搭配的适用于 Server 新生代收集器

可以配合 CMS & SerialOld 老年代收集器一起工作

##### Parallel Scavenge 收集器

ParNew 负责回收新生代内存中的对象, 采用复制算法

ParNew 是 Serial 收集器的多线程版本, 除了并行(指的是多个垃圾回收线程并行执行, 不过同样要暂停用户线程)的特性外, 其余都一致

在单 CPU 的条件下, 回收效率没有 Serial 高, 但是在并发情况下效果更好

ParNew 用于 Server 端, 是唯一款可以和 CMS 搭配的适用于 Server 新生代收集器

可以配合 CMS & SerialOld 老年代收集器一起工作

#### 老年代

##### CMS 收集器

CMS 属于老年代垃圾回收器, 采用标记 - 清除算法

CMS 的目标是尽可能的减少回收停顿时间, 从而提高用户的响应时间和体验度

CMS 是一款并发的垃圾收集器, 在它的并发标记与并发清除阶段, 不需要暂停用户线程

CMS 工作分作四个步骤

1. 初始标记(Stop the world) 只需要简单标记一下 GC Root 能直接关联到的对象
2. 并发标记
3. 重新标记(Stop the world)
4. 并发清除
   其中时间开销 T(并发标记) > T(重新标记) > T(初始标记)

由于整个过程中耗时最长的并发标记阶段和并发清除阶段过程, 垃圾收集器都可以与用户线程一起工作, 所以总体上来讲 CMS 收集器的回收过程是和用户线程并发工作的

缺点有三点

1. CMS 对 CPU 资源敏感, 因为与用户线程一起工作, 所以会抢占用户线程的 CPU 资源, 导致用户线程的执行效率下降
2. CMS 无法收集浮动垃圾, 可能会导致 Concurrent Mode Failure 失败而导致另一次 Full GC 的产生. 浮动垃圾是当垃圾回收器在运行的适合产生的新的垃圾. Concurrent Mode Failure 是当 CMS 运行期间, 预留给用户的内存不够, 这是 JVM 会启动备用方案: 使用 Serial Old 垃圾收集器来回收老年代, 这个适合就需要 Stop the world, 所以预留给用户的内存比例不能太小
3. CMS 基于标记 - 清除算法实现的, 所以会产生很多空间碎片

CMS 适用于 Server 端, 需要重视服务的响应速度, 系统停顿时间最短的应用

CMS 可以搭配 Serial & ParNew 年轻代垃圾回收器一起工作

##### Serial Old 收集器

Serial Old 负责回收老年代内存当中的对象, 采用标记 - 整理算法

Serial Old 适合于 Client 客户端, 但在 Server 端也有两大用处

1. 在 JDK 1.5版本之前搭配 Pallel Scavenge 收集器使用
2. 当 CMS 并发收集发生 Concurrent Mode Failure 的适合, 作为备用方案

可以配合 Serial & ParNew & Parallel Scavenge 年轻代收集器一起工作

##### Parallel Old 收集器

Parallel Old 负责老年代的对象回收, 采用标记 - 整理的算法

Parallel Old 是 Parallel Scavenge 的老年代收集器, 同样关注高吞吐量

只能配合 Parallel Scavenge 完成工作, 针对交互少, 需要大量计算的任务

### 什么时候触发 GC

由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：Scavenge GC 和 Full GC。

#### Scavenge GC

一般情况下，当新对象生成，并且在 Eden 申请空间失败时，就会触发 Scavenge GC，对 Eden 区域进行 GC，清除非存活对象，并且把尚且存活的对象移动到 Survivor 区。然后整理 Survivor 的两个区。这种方式的 GC 是对年轻代的 Eden 区进行，不会影响到年老代。

#### Full GC

对整个堆进行整理，包括Young、Tenured和Perm。Full GC因为需要对整个对进行回收，所以比Scavenge GC要慢，因此应该尽可能减少Full GC的次数。在对JVM调优的过程中，很大一部分工作就是对于FullGC的调节。

触发 full GC

1. 年老代（Tenured）被写满
2. 持久代（Perm）被写满
3. System.gc()被显示调用
4. 垃圾收集器自定义的策略被激活

### 什么情况下影响新生代回收速度

1. Eden 与 Survivor 分区大小的比例，当 Survivor 很小的时候，可能会频繁的引起新生代的回收

2. 较大的 Eden 与 Surviror 也会影响到新生代回收需要扫描的对象的数量

## 类加载

### 类加载过程

类加载由 5 个阶段的动作组成：加载、验证、准备、解析、初始化

1. 加载

主要做三件事情：1.获取一个类的二进制字节流。2.把这个字节流的数据转化成运行时数据结构。3.在内存当中生成一个代表这个类的 Class 对象，用于方法区对这个类的各种数据的访问入口

不一定非得要从一个Class文件获取，这里既可以从ZIP包中读取（比如从jar包和war包中读取），也可以在运行时计算生成（动态代理），
也可以由其它文件生成（比如将JSP文件转换成对应的Class类）。

2. 验证

验证主要是为了保证 Class 文件字节流当中的信息是符合 JVM 要求的不会危害 JVM 自身安全的。也就是要检查是否访问数组边界以外的数据、强制转换了一个不存在的类等等

3. 准备

准备阶段是为 类变量 分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。

注意 `public static int value = 123` 在这个阶段初始化后为 0 而不是 123，123 是在编译阶段 putstatic 指令被运行的时候才赋值

4. 解析

解析是把 JVM 常量池当中的符号引用替换为直接引用的过程

符号引用：符号引用以一组符号来描述锁引用的目标，可以是任何形式的字面量，只要能无歧义的定位到一个目标即可

直接引用：直接引用是直接指向目标的指针或者句柄

5. 初始化

初始化是真正的执行类当中定义的 java 程序代码

在准备阶段，变量已经按照系统的要求赋值过一次，但是在初始化阶段，可以通过程序指定计划去初始化类变量和其它资源

注意以下几种情况不会执行类初始化：

1. 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化。
2. 定义对象数组，不会触发该类的初始化。
3. 常量在编译期间会存入调用类的常量池中，本质上并没有直接引用定义常量的类，不会触发定义常量所在的类。
4. 通过类名获取Class对象，不会触发类的初始化。
5. 通过Class.forName加载指定类时，如果指定参数initialize为false时，也不会触发类初始化，其实这个参数是告诉虚拟机，是否要对类进行初始化。
6. 通过ClassLoader默认的loadClass方法，也不会触发初始化动作。

### 类加载时机

JVM 当中没有明确规定什么时候执行加载过程，只规定了五种情况必须要初始化

1. 遇到 new getstatic putstatic invokestatic 字节码是，如果没有初始化，一定要初始化

也就是当 new 关键字示例化对象的时候，读取或者设置一个类的静态字段的时候以及调用一个类的静态方法的时候

2. 使用 reflect 反射机制的时候，如果类没有进行过初始化，则先触发其初始化

3. 当初始化一个类的时候，如果其父类还没有初始化，要先初始化它的父类

4. 虚拟机启动的时候，初始化 main 方法的类

### 双亲委派模型

双亲委派模型的工作过程是，如果一个类加载其收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求给它的父类去实现，每一个层次的类加载器都是如此，只有当父类加载器反馈自己无法完成这个加载请求的时候，子加载器才会自己去加载。

好处：Java 类随着它的类加载器一起具备了一种层次关系，例如 Object，无论任何一个加载器来加载都应该是启动类加载器来加载，而不是由不同的加载器来加载，如果由不同的加载起来加载就会产生很多个不同的 Object 类(不同的类加载器加载同一个类不认为是相同的类)

# MySQL

## 事务

### 什么是事务

事务是一系列 Sql 操作，也可以叫做一个独立的工作单元，该工作单元封装了一系列的工作流程，该流程当中的每个步骤都必须要全部成功，如果有一个步骤失败，则整个工作流程都算失败。

### 事务举例

假设一个银行的数据库当中有两张表，**支票(checking)表** & **储蓄(caving)表**, 现在要从用户 Jane 的支票表当中转移 200 美元到他的储蓄账户当中，那么事务处理的步骤至少应该做这些事情

1. 检查支票账户的余额，如果低于 200 那么这个步骤失败
2. 从支票账户当中减去 200 美元
3. 在储蓄账户当中增加 200 美元

以上的三个步骤组合起来叫做 **事务**，任何一个步骤失败，则必须回滚所有的步骤。

### 事务的4个特性

事务有4个特性：原子性、隔离性、持久性、一致性。

### 原子性

在我们操作数据库的时候，事务是做一个不可分割的最小的工作单元，整个事务的所有步骤，要么全部成功，要们就全部失败，不可能只执行其中的某一部分操作。

### 隔离性

通常来讲，一个事务所做的修改，在最终提交前，对其他的事务都是不可见的。

### 持久性

一旦事务提交，那么所做的修改就会永远的保存在数据库当中。

### 一致性

执行业务操作的时候，会预先规定一些规定，在进行事务的时候，必须要从一个满足这些规定的状态到另外一个满足这个状态中去，
例如规定转账的时候支票账户和储蓄账户相加的总数要为原来的事务开始时候的状态，同时按照预先规定的流程执行事务，事务执行完后，同样要满足预先设定的业务规定。

## 隔离级别

Sql 标准当中定义了 4 种隔离级别，分别是 **未提交读(Read committed)** **提交读/不可重复读(Read Committed)** **可重复度(Repeateable Read)** **可串行化**

隔离级别规定了一个事务当中所做的修改对于系统当中的其它 session 的可见程度的高低

大部分的数据库默认级别是 **Read Committed**, mysql 当中的默认级别是 **Repeateable Read**

ps: 在数据库当中执行 ```select @@session.tx_isolation;``` 可以查询 **事务的隔离级别**, 在数据库当中执行 ```set session transaction isolation level serializable;``` 可以更改数据库隔离级别, 但是时间只是在这次 client 打开的时候, 当再次登录时, 隔离级别会重置

#### 未提交读

事务当中的修改, 即使没有提交, 其它事务能看见这个事务当中的修改操作

这个级别当中不经不能保证事务的一致性, 而且本身能够得到的性能优化也非常的有限

##### 案例

例如有两个 session 分别为 session A & session B, 还有一个表 **status(id int, number id)**

实验步骤为：打开两个事务 -> 事务 A 插入数据 **id = 1 & number = 10** 并且不提交 -> 在事务 B 当中查询 **id = 1** 可以查到该插入记录

打开两个事务，在 client A 当中执行 ```start transaction```, 在 client B 当中执行 ```start transaction```

client A 当中执行代码 ```insert into status (id,number) values (1,10)``` 插入数据， 在 client B 当中执行查询 ```select id,number from status where id = 1;```

按照事务隔离等级的规则, 事务 B 在事务 A 提交之前或提交之后都可以查看到 session A 插入的结果

#### 提交读/不可重复读

提交读的意思是，当一个事务当中执行操作，只有当当前的这个事务提交之后，别的事务才能看到这个事务执行后的数据更新效果。

提交读又叫做不可重复度，不可重复读的意识是可能会在读取当中由于另外事务 session A 提交了数据的更改操作，导致没有办法再读取到 session A 提交之前的数据。

###### 案例

例如有两个 session 分别为 session A & session B, 还有一个表 **status(id int, number id)**

实验步骤：打开两个事务 -> 事务 A 插入数据 **id = 2 && number = 20** -> 事务 B 执行查询 -> 事务 A 提交事务 -> 事务 B 执行查询

打开两个事务，在 client A 当中执行 ```start transaction;```, 在 client B 当中执行 ```start transaction;```

client A 当中执行代码 ```insert into status (id, number) values (2,20)``` 插入数据, 在 client B 当中执行查询 ```select id,number from status where id = 2;``` 再在 client A 当中提交事务 ```commit;```, 当事务 A 提交过后在 client B 当中再次执行查询 ```select id,number from status where id = 2;```

按照事务隔离等级的规则, 事务 B 无法查看到事务 A 执行 ```commit;``` 命令之前的插入数据, 但是在事务 A 执行完提交语句后 ```commit;```, 事务 B 可以查询到事务 A 执行的插入语句

#### 可重复读

可重复读的意思是, 一个事务如果打开了, 那么就算别的事务提交了对数据的修改, 这个事务依旧可以读取到打开时事务的数据, 直到它本身提交后, 才会读取到新的被别的事务修改过后的数据。

##### 案例

例如有两个 session 分别为 session A & session B, 还有一个表 **status(id int, number id)**

实验步骤: 打开两个事务 -> 事务 A 插入数据 **id = 3 & number =30** -> 事务 B 执行查询 -> 事务 A 提交事务 -> 事务 B 再次执行查询

打开两个事务，在 client A 当中执行 ```start transaction;```, 在 client B 当中执行 ```start transaction;```

client A 当中执行代码 ```inser into status (id, number) values (3, 30)``` 插入数据, 在 client B 当中执行查询 ```select id,number from status where id = 3``` 再在 client A 当中提交事务 ```commit;```, 当事务 A 提交过后在 client B 当中再次执行查询 ```select id,number from status where id = 3```

按照事务隔离等级的规则, 事务 B 无法查看到事务 A 插入的数据, 即使事务A 提交了事务, 事务 B 也没有办法查看到事务 A 插入的数据.

#### 可串行化

最高的隔离级别

可序列化的意思是, 数据库在同步控制当中, 强制的在同一时间内只能有一个事务进行读或取操作(具体看锁的类型), 也就是说当多个事务对同一数据进行操作的时候, 需要去获取锁才可以

实验步骤: 打开两个事务 -> 事务 A 执行插入语句 ```insert into status (id,number) values (4,40)``` -> 事务 B 执行查询 -> 事务 A 更新数据 ```update status set number = 44 where id = 4;``` -> 事务 B 执行更新语句 ```update status set number = 444 where id = 4;```

按照事务隔离等级的规则, 事务 B 的更新语句无法执行, 只有当事务 A 提交了事务后, 事务 B 的更新语句才能执行

#### 隔离级别的读问题

这里说的读问题, 指的是不同的隔离级别的条件下, 许多个事务同时执行读写操作的时候, 会产生的一些问题, 所以读问题都是建立在事务隔离级别之下来讨论的

一般来讲会有三种读问题: **脏读** **不可重复读** **幻读**

事务的隔离等级和读问题的关系

| 隔离级别 | 脏读可能性 | 不可重复读可能性 | 幻读可能性 | 加锁读 |
| -------- | ---------- | ---------------- | ---------- | ------ |
| 未提交读 | yes        | yes              | yes        | no     |
| 提交读   | no         | yes              | yes        | no     |
| 重复读   | no         | no               | yes        | no     |
| 可串行化 | no         | no               | no         | yes    |

##### 脏读

脏读又叫做无效数据读出

脏读指的是在数据访问当中, 事务 T1 将某一值进行修改但是还没有提交, 存在事务 T2 读取该数据 && 可以读取到 T1 修改但是没有提交的值, 然后 T1 运行时有步骤失败需要回滚事务, 这个时候事务 T2 读取到的值就是脏数据, 依照脏数据所做的操作可能是不正确的

脏读一般发生在: 多个事务的内部修改对其他事务来讲是可见的。

##### 不可重复读

不可重复读指的是, 假设事务 T1 第一次读取某一数据结果集为 set A, 在事务 T1 读取完后, 有第二个事务 T2 读取并且了该数据然后提交事务, 事务 T1 再次执行与第一次读取相同的代码再次得到一个结果集 set B, 这个时候, set A 当中的数据和 set B 当中的数据不一致, 也就是在别人修改并且提交事务后, T1 没有办法再读取到和第一次读取时相同的数据集, 就叫做不可重复读

不可重复读是由于事务的交叉提交而导致的

##### 幻读

幻读是建立在解决了不可重复读的问题之上的, 解决不可重复读是让这个事务在执行的时候, 所有的读取操作都是事务开始的时候的一个状态, 所以在执行操作完成后再次执行相同的查询就会产生不同的结果集, 这样就会导致如果我的操作是根据结果集的内容来做修改或者判断的依据, 那么当别的事务提交新的数据的时候, 在执行操作的这个业务不会感知到, 从而导致有一部分数据就像是幻觉一样

## 事务的实现

#### undo log

undo 日志用于存放数据被修改前的值。

假设修改 a 表中 id=2 的行数据，把 Name='B' 修改为 Name = 'A' ，那么 undo 日志就会用来存放 Name='B' 的记录，如果这个修改出现异常，可以使用 undo 日志来实现回滚操作，保证事务的一致性。

undo log 是逻辑日志, 根据每行的记录进行记录, 恢复事务对数据库数据的修改。

#### redo log

insert 一条记录时, 插入的信息都会放进 redo 中, 在  commit 之前, redo 的信息会放进硬盘上。故障时, redo 便可恢复那些已经 commit 了的数据。

每次操作都先记录到 redo 日志中，当出现实例故障（像断电），导致数据未能更新到数据文件，则数据库重启时须redo，重新把数据更新到数据文件。

redo log 有一个很特别的地方，就是 Redo log 以顺序的方式写入文件文件，写满时则回溯到第一个文件进行覆盖重写。

redo log 是物理日志, 记录的是物理的修改记录, 由于特殊原因数据丢失需要恢复数据库之前的数据。

#### undo log 与 redo log 的 IO 影响

Undo 搭配 Redo 的设计主要考虑的是提升 IO 性能，增大数据库吞吐量。只有当事务要提交之前才会进行 IO 操作，之前的更新操作都是写到 redo 的缓冲区当中，这样就保证 Redo Log 能够有比较好的 IO 性能，InnoDB 的 Redo Log 的设计有以下几个特点：

A. Redo Log 存储在一段连续的空间上。因此在系统第一次启动时就会将日志文件的空间完全分配。 以顺序追加的方式记录 Redo Log，这样可以减少硬盘指针寻址的时间。

B. 批量写入日志。日志并不是直接写入文件，而是先写入 redo log buffer。当需要将日志刷新到磁盘时(如事务提交)，再将缓冲区的日志一起写入磁盘。

C. 并发的事务共享 Redo Log 的存储空间，它们的 Redo Log 按语句的执行顺序，依次交替的记录在一起，以减少日志占用的空间。例如,Redo Log中的记录内容可能是这样的：
     记录1: <trx1, insert …>
     记录2: <trx2, update …>
     记录3: <trx1, delete …>
     记录4: <trx3, update …>
     记录5: <trx2, insert …>

D. 因为多个事务同时都可以写入 Redo log 的原因,当一个事务将 Redo Log 写入磁盘时，也会将其他未提交的事务的日志写入磁盘。

E. Redo Log上只进行顺序追加的操作，当一个事务需要回滚时，它的 Redo Log 记录也不会从 Redo Log 中删除掉。

#### undo log 与 redo log 与数据恢复

MySQL 数据库 InnoDB 存储引擎使用的策略是这样的：进行恢复时，重做所有事务包括未提交的事务和回滚了的事务。然后通过 Undo Log 回滚那些 未提交的事务。

InnoDB 存储引擎中的恢复机制有几个特点：

A. 在重做 Redo Log 时，并不关心事务性。 恢复时，没有 BEGIN，也没有 COMMIT,ROLLBACK 的行为。也不关心每个日志是哪个事务的。尽管事务 ID 等事务相关的内容会记入 Redo Log，这些内容只是被当作要操作的数据的一部分。

B. 使用B策略就必须要将Undo Log持久化，而且必须要在写Redo Log之前将对应的Undo Log写入磁盘。Undo和Redo Log的这种关联，使得持久化变得复杂起来。为了降低复杂度，InnoDB将Undo Log看作数据，因此记录Undo Log的操作也会记录到redo log中。这样undo log就可以象数据一样缓存起来，而不用在redo log之前写入磁盘了。

包含Undo Log操作的Redo Log，看起来是这样的：
    记录1: <trx1, Undo log insert <undo_insert …>>
    记录2: <trx1, insert …>
    记录3: <trx2, Undo log insert <undo_update …>>
    记录4: <trx2, update …>
    记录5: <trx3, Undo log insert <undo_delete …>>
    记录6: <trx3, delete …>

C. 因为 Redo 没有事务性，所以会重新执行被回滚了的事务，同时 Innodb 也会将事务回滚时的操作也记录到 redo log 中。因为回滚操作本质上也是对数据进行修改，因此回滚时对数据的操作也会记录到 Redo Log 中。

一个回滚了的事务的 Redo Log，看起来是这样的：
    记录1: <trx1, Undo log insert <undo_insert …>>
    记录2: <trx1, insert A…>
    记录3: <trx1, Undo log insert <undo_update …>>
    记录4: <trx1, update B…>
    记录5: <trx1, Undo log insert <undo_delete …>>
    记录6: <trx1, delete C…>
    记录7: <trx1, insert C>
    记录8: <trx1, update B to old value>
    记录9: <trx1, delete A>

*！* 一个被回滚了的事务在恢复时的操作就是先redo再undo，因此不会破坏数据的一致性。

#### 原子性的实现

回滚日志来实现。

数据库当中的事务在执行的时候, 事务对数据的修改都会记录相反的操作到 undo log 日志当中同时会记录修改的记录到 redo log 日志当中, 如果事务需要回滚, 那么就会依照这个修改日志来回滚, 如果没有问题, 就会对数据对应的行进行写入操作。

#### 隔离性的实现

隔离性的实现依赖于数据库的并发控制, 我所了解到的并发控制有两种: **锁** **多版本/快照隔离**

#### 一致性的实现

使用 redo log 来实现, 当事务没有完整提交的时候，可以通过 redo log 来重写的。

#### 持久性的实现

根据数据库的 redo log 来保障的, redo log 可以在数据库崩溃的时候恢复数据库当中的数据。

存储引擎

#### InnoDB

Innodb引擎提供了对数据库ACID事务的支持，并且实现了SQL标准的四种隔离级别。该引擎还提供了行级锁，它的设计目标是处理大容量数据库系统，MySQL运行时Innodb会在内存中建立缓冲池，
用于缓冲数据和索引。但是有一个小细节是，它没有保存表的行数，当SELECT COUNT(*) FROM TABLE时需要扫描全表。当需要使用数据库事务时，该引擎当然是首选。
由于锁的粒度更小，写操作不会锁定全表，所以在并发较高时，使用Innodb引擎会提升效率。但是如果在执行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表。

#### MyISAM 

MyIASM是MySQL默认的引擎，但是它没有提供对数据库事务的支持，也不支持行级锁，因此当INSERT(插入)或UPDATE(更新)数据时即写操作需要锁定整个表，效率便会低一些。
不过和Innodb不同，MyIASM中存储了表的行数，于是SELECT COUNT(*) FROM TABLE时只需要直接读取已经保存好的值而不需要进行全表扫描。
所以，如果表的读操作远远多于写操作且不需要数据库事务的支持，那么MyIASM也是很好的选择。

#### 应用场景

1. MyIASM管理非事务表，提供高速存储和检索以及全文搜索能力，如果再应用中执行大量select操作，应该选择MyIASM
2. InnoDB用于事务处理，具有ACID事务支持等特性，如果在应用中执行大量insert和update操作，应该选择InnoDB

## 索引

#### 什么是索引

索引是一个类似与目录的东西, 它能够指明所需要查找的数据的位置。

#### 索引的优点

索引的具体功能是基于其实现方式的数据结构的特性, 例如 B-Tree 当中的数据是有序的, 那么基于 B-Tree 的索引就具有排序快这一优点, 但具体有三点

- 大大减少服务器需要扫描的数据量
- 帮助服务器避免排序和临时表
- 将随机 I/O 变成顺序 I/O

#### 索引的工作原理

索引类似于书本的目录, 想要找到一个特定的数据时候, 一般先回去找到这个索引存放的指向具体数据的指针, 再返回想要找到的数据。

#### 哈希索引

哈希索引是基于哈希表实现的, 对与每一行数据, 存储引擎都会对所有的索引列计算一个哈希码, 不同的哈希码也不一样, 哈希索引将所有的哈希码存储在索引当中, 同时也创建一个哈希表, 在这个哈希表当中保存指向每个数据行的指针, 它适合于精确匹配查找, 与 B-Tree 不同的是非常的快

Mysql 当中, 只有 Memory 引擎现实支持哈希索引, Memory 引擎也是默认把哈希索引当中索引类型

##### 哈希的有效查询

精确匹配索引的所有列的查询

##### 限制

- 必须要整个索引键才能唯一确定一行
- 没有办法提高 ORDER BY 操作
- 哈希索引一定会读取数据行, 因为哈希索引只保存了哈希值和行指针, 不会保存字段值
- 哈希索引并不按照索引值顺序存储的, 索引也就无法用于排序
- 哈希索引是把所有的索引列都用来计算才能得到唯一的哈希值, 所以也就没有办法使用部分索引列匹配查询
- 哈希索引只支持等值比较查询, 不能做范围查询
- 如果哈希冲突很高的话, 那么就需要遍历所有这个索引值可能生成的哈希值

##### InnoDB 使用哈希索引的优化

InnoDB 的索引使用 B-Tree 来实现, 但是如果引擎发现某一个索引值会被频繁的访问到, 那么就会计算它的哈希码, 然后基于 B-Tree 的数据结构生成一个哈希索引, 这样就具备了一些哈希的基本优点

例如: 当有一个表存放了一些关于网页的信息`id, url, name, context`, 使用 B-Tree 的查找方法一般为 `select id from table where url='http://www.mysql.com'`, 如果这个时候表很大查找起来就会很慢, 这个时候如果新加入一列 url 的哈希码使表变成 `id, url ,name ,context, hash`, 并且使用这个查找方法`select id from table where url='http://www.mysql.com' and hash=CRC32('http://www.mysql.com')`, 那么引擎会优先根据选择性很高而且体积很小的一列也就是 **hash** 这一列来查找, 这个时候就会得到一些索引条目, 然后一一比较返回对应的行, 就可以得到内容

*!* CRC32() 返回32位整数, 当索引条目为 93000 条记录的时候, 出现冲突的概率为 1%

#### B-Tree 索引

B-Tree 常用来实现 **数据库** 和 **文件系统** 的一个数据结构, 它能保持数据的有序性, 很适合用来查找范围类型的数据

##### B-Tree 工作原理

例如有一颗 **m 阶 B-Tree** 从根结点开始插入数据, 每一节点当中保存 **键值** **指针** **数据** 三种类型的数据, **键值** 就是数据库当中的索引数据, **指针** 指向的该节点的孩子, **数据** 则是指向数据存放地址的一个句柄, 每一个节点最多只能有 m 个孩子, 并且在插入的时候对比当前的键值的大小决定是否在当前的节点插入或者插入该节点的那个孩子, 如果是查询, 就会从根结点开始, 同样是对比范围, 然后确定查找的范围, 最后要么查找到该数据, 要么就是空值

##### 有效查询

B—Tree 索引适用于全键值、键值范围、最左前缀查找

- 全值匹配 和索引中的所有的列进行匹配
- 匹配最左前缀 匹配索引当中的第一列数据
- 匹配列前缀 只匹配某一列的值的开头部分, 例如 J 开头的姓的人
- 匹配范围值 查找某个范围内的值
- 精确匹配某一列并范围匹配另外一列 例如查找所有姓为 Allen, 并且名字为字母 K 开头的人
- 只访问索引的查询 查询的过程值需要防伪索引, 而无需访问数据行

##### 限制

- 如果不是从最左边的列开始查找, 则无法使用索引
- 不能跳过索引中列, 也就是匹配的过程要按照索引定义的列的顺序来匹配
- 在一个索引集合当中, 其中一个查询有某个列的范围查询, 则这个列右边的所有的列都无法使用索引优化技术

##### 特性

1. 每个节点最多有m个孩子。
2. 除了根节点和叶子节点外，其它每个节点至少有Ceil(m/2)个孩子。
3. 若根节点不是叶子节点，则至少有2个孩子
4. 所有叶子节点都在同一层，且不包含其它关键字信息
5. 每个非终端节点包含n个关键字信息（P0,P1,…Pn, k1,…kn）
6. 关键字的个数n满足：ceil(m/2)-1 <= n <= m-1
7. ki(i=1,…n)为关键字，且关键字升序排序。
8. Pi(i=1,…n)为指向子树根节点的指针。P(i-1)指向的子树的所有节点关键字均小于ki，但都大于k(i-1)

#### 唯一索引

唯一索引所在的列的所有的值都不能被重复

系统在创建该索引时检查是否有重复的键值，并在每次使用 INSERT 或 UPDATE 语句添加数据时进行检查。

##### 书写方法

```sql
CREATE UNIQUE CLUSTERED INDEX myclumn_cindex ON mytable(mycolumn)
```

CLUSTERED INDEX是用来建立聚簇索引的关键字。

此语句的意思是在表mytable上的mycolumn字段上创建一个名为myclumn_cindex的聚簇索引，且为唯一索引。

#### 索引优化

##### 独立的列

在使用 where 语句的时候, `=` 号前边的索引不能有运算符或者是作为函数的参数, 不然 sql 语句没有办法解析, 也就没有办法使用索引

例如: `select id from actor where id + 1 = 5;` 这个时候就没有办法解析 `id + 1 = 5`, 应该改成 `select id from actor where id = 4;`

##### 前缀索引

当需要对类似于 text 或者很长的 varchar 这样的大文本建立索引的时候, 如果不做处理就建立索引, 就会让索引变得很大而且很慢而且 mysql 也不支持索引这些列的完整长度, 前缀索引就是把这一列的前面一部分数据当作索引, 例如一篇文章的开始的 10 个字当作这一行的索引数据, 选择多长的数据当作索引, 同时需要考虑效率和索引的选择性

*!* 索引的选择性指的是 **不重复的索引值**/**数据表的记录总数**, 选择性越高效率就越高, 因为选择性高的索引可以让 mysql 在查找的时候过滤掉更多的行

*!* 唯一的索引的选择性为1

*!* 一般来讲, 前缀长度的大小取决于 **最常出现的前缀的出现次数** 与 **最常出现的索引行数据的次数** 相当或者再加 1 选择性的提升不大

##### 多列索引

如果在一张表里面需要利用多个列来搜寻唯一一行数据, 那么我们有三个方法 **全表扫描** **建立多个单列索引** **建立一个多列索引**

**全表扫描** 的缺点是效率不高

**建立多个单列索引** 在使用多个单列索引查找的时候, 会有一个索引合并的操作, mysql 可以把它换做成 OR 条件 & AND 条件 & OR 和 AND 条件的合并, 但是由于需要对每一个列都建立索引, 那么就会使得索引膨胀很快

**建立多列索引** 多列索引把多个列建立成索引, 建立多个索引的查询是根据索引定义的第一个列开始查找, 然后当第一个列的值相同的时候, 就比较第二列的值, 以这样的方式来查找到数据, 当我们建立了 (A,B,C) 索引的时候相当于建立了 (A,B,C) & (A,B) & (A) 三个索引

###### 多列索引的顺序

建立多列索引需要考虑到索引列的顺序问题, 需要通过 **具体的场景** + **索引的选择性** + **全局基数** 来衡量

**具体的场景** 指的是索引应该考虑到的例如 **排序** **分组** 这样的特殊情况, 例如当我们使用 B-Tree 当作索引的时候, 它的排序是先匹配第一列再匹配第二列, 所以这个时候, 列的顺序就很重要了, 这直接影响到排序的结果

**索引的选择性** 指的是我们在不考虑具体场景的情况下, 应该把选择性高的一列放在索引当中的前面些的位置, 因为这样的话, 能够最快的过滤出所需要的行, 同时对于在 where 子句当中只是用了索引部分的前缀列的查询来说选择性也更高

**全局基数** 指的是当一个索引存在特殊值, 根据这个特殊值查询到的结果集很大时, 就需要对这个索引做特殊的处理

##### 覆盖索引

是指如果查询的列恰好是索引的一部分，那么查询只需要在索引文件上进行，不需要回行到磁盘再找数据，这种查询速度非常快，这个现象称为“索引覆盖”。

- 极大减少查询数据对磁盘的访问次数

- 如果二级主键能够覆盖查询, 可以避免回表操作

##### 聚簇索引

聚簇索引是按照索引顺序把数据行存储在磁盘上的一种索引, 这样做有几个好处 1.优化范围查询(因为它把相邻的数据存放在一起了) 2.有一个默认的顺序

- 索引的顺序按照主键的顺序来规定
- 聚簇索引不是作为数据库当中的一个单独的对象/结构存在, 换一种话来说, 聚簇索引实际是一个表
- 聚簇索引当中的索引叶不仅仅存放 **索引键** 它还会存储整个行数据

好处：聚簇索引通常要比非聚簇索引快, 因为它在索引表当中存储了数据, 比起非聚簇索引它会少一个 I/O 操作

限制：由于聚簇索引按照一定的规则对数据进行了排序, 那么在每次 **插入** 与 **删除** 的时候都需要对树当中的位置进行调整

- 插入: 插入的效率依赖与插入的顺序, 按照主键的顺序来插入是最快的
- 删除: 删除可能会导致下方的数据向上移动来填充删除掉的数据产生的空白位置, 这样会有大量的移动操作
- 更新: 当更新主键的时候, 会重新的调整主键的位置, 所以一般不会更新主键

##### 压缩索引

MyISAM 使用前缀压缩来减少索引的大小, 一般只默认压缩字符串, 但也可以压缩数字, 压缩每个索引块的方法是, 先完全保存索引块当中的第一个值, 然后其它值和第一个值进行比较, 得到一个相同的前缀和一个不同的后缀, 然后在用前缀和后缀替换掉原来的数据

例如: 索引块当中第一个值是 `perform` 第二个值是 `performance`, 那么第二个值压缩过后就是 `7,ance`

压缩索引可以让索引所占的空间变得很小, 但是会降低查询的速度, 尤其是在 **随机查询** 和 **高密度 I/O** 的应用当中

##### 减少冗余、重复、未使用的列

减少冗余、重复、未使用的列

## 并发控制

#### 什么是并发控制

并发控制主要是为了保障事务当中的 **隔离性** 而存在的, 控制并发的方式有两种: 1.使用锁 2.使用版本控制

关于锁, mysql 对于锁的分类有很多种方式: **按照处理并发的策略叫做: 悲观锁 & 乐观锁** **按照锁的粒度来分: 表锁 & 页锁 & 行锁**

关于版本控制有两种实现方式 **系统版本号 & 时间戳**

#### 锁

##### 锁的分类

按粒度来分：表锁 & 页锁 & 行锁

按策略来分：悲观锁 & 乐观锁

##### 粒度锁

###### 各存储引擎的支持

- MyISAM & MEMORY 支持 **表级锁**
- BDB 支持 **页面锁( 默认 )** **表级锁**
- InnoDB 支持 **行级锁( 默认 )** **表级锁**

###### 锁的优劣

- 表级锁: 开销大, 加锁快; 不会出现死锁; 锁定粒度大, 发生锁冲突的概率最高, 并发程度最低
- 行级锁: 开销大, 加锁慢; 会出现死锁; 锁定粒度最小, 发生锁冲突的概率最低, 并发读页最高
- 页面锁: 开销和加锁时间介于表锁和行锁之间; 会出现死锁; 锁定粒度介于表锁和行锁之间, 并发度一般

###### MyISAM 表级锁

MyISAM 只支持表级锁, 所以在这个存储引擎当中表级锁有两个工作模式 **表共享锁 & 表独立锁**

当一个事务想要对表进行读操作的时候, 就会获得一个读锁, 读锁是共享的, 读锁和读锁之间不会相互排斥, 执行 **SELECT** 语句的时候会自动给涉及的表加 **读锁**

当一个事务想要对表进行写操作的时候, 就会要求获得一个写锁, 写锁是独占的, 会阻塞其它事务获取读锁和写作, 同时也会阻塞其它事务的读写操作, 执行 **UPDATE** **DELETE** **INSERT** 语句的时候会自动给涉及的表加 **写锁**

###### MyISAM 表级锁的调度

MyISAM 认为 **写操作** 比 **读操作** 更加重要

如果两个事务同时分别请求一个读锁和一个写锁, 那么优先分配该表的写锁给写锁事务

所以 MyISAM 更适合读操作比较密集 & 写操作比较少的表, 因为如果存在大量的写操作, 那么写会优先获得锁, 那么读操作就很难获得读锁, 从而可能永远阻塞

*！* 在长时间调节读写优先级的时候应该注意: 长时间的 **读进程** 可能会导致 **写进程** 饿死, **长时间** 的写进程可能也会导致 **读进程** 永远阻塞

###### MyISAM 的并发插入

并发插入是为了解决读和插入写操作串行执行导致效率过低的问题, 所以 Mysql 提供一个系统变量 **concurrent_insert** 来控制在什么样的条件下, 允许在表的末尾插入的数据

比如一个 session 获取了 demo 表的 READ LOCAL 锁, 该线程可以对表进行查询炒作, 但不能对表进行更新操作, 如果我们允许插入的话, 其它的线程就可以根据 **concurrent_insert** 变量的值来对 demo 表进行并发插入操作

###### MyISAM 表级锁兼容性

    //是否兼容
         | None | 读锁  | 写锁  |
    读锁  |  是   |  是  |  否  |
    写锁  |  是   |  否  |  否  |
###### InnoDB 锁支持

InnoDB 支持 **行级锁** **表级锁** , 但是 **行级锁** 是默认采用的

InnoDB 和 MyISAM 的主要区别 **InnoDB 支持事务** **InnoDB 采用了行级锁**

###### InnoDB 锁分类

两种行锁

- 共享锁 S
- 排他锁 X

*！* 对于 UPDATE & DELETE & INSERT, InnoDB 会自动给设计的数据集加 **排他锁(X)**
*！* 对于普通的 SELECT 语句, InnoDB 不会加任何、

两种内部使用的意向锁 && 表锁

- 意向共享锁 IS (事务打算给数据行加行共享锁, 那如果想给一个数据行加共享锁钱必须先取得该表的 IS 锁)
- 意向排他锁 IX (事务打算给数据行加行排他锁, 那如果想给一个数据行加排他锁就必须先取得该表的 IX 锁)

*！* 意向锁都是 InnoDB 自动加的, 不需要用户干预

###### InnoDB 行锁兼容性

```
    //是否兼容, 能同时获取到锁并操作表
       |  X  |  IX  |  S  |  IS  |
    X  |  N  |  N   |  N  |   N  |  
    IX |  N  |  Y   |  N  |   Y  |  
    S  |  N  |  N   |  Y  |   Y  |  
    IS |  N  |  Y   |  Y  |   Y  |  

```

###### InnoDB 行锁的实现

InnoDB 的行锁是通过给索引上锁来实现的, 具体分为 **有索引**  **无索引**

- **有索引** 给索引上锁
- **无索引** 通过隐藏的聚簇索引来对记录加锁

###### InnoDB 行锁的三种情况

- Record Lock 对索引项上锁，所以说当一条sql没有走任何索引时，那么将会在每一条聚集索引后面加X锁，这个类似于表锁，但原理上和表锁应该是完全不同的。
- Gap Lock 对索引项之间的'间隙' & 第一条记录前的'记录' & 最后一条记录后的‘间隙’ 加锁
- Next-key Lock 前两种的组合, 对记录和前面的'间隙' 上锁

*！* 间隙: 值得是通过一个条件范围获取一堆数据, 数据库当中没有的数据但是却数据条件范围内的就叫做间隙. 例如: `select * from emp where emp_id > 100 for update` 如果数据库当中有 **1-101** 的数据项, 那么 **102** 就属于间隙, 间隙存在是为了防止幻读

##### 策略锁

###### 悲观锁

悲观锁是以加锁的方式来独占一个数据资源, 从而防止另外的事务在该事务更新或者操作数据的时候改变了数据的状态

数据安全性高、并发效率低

###### 悲观锁的处理流程

1. 线程读取某条数据
2. 线程立刻尝试给该数据上锁
3. 等待获取该数据的锁的权限
4. 加锁成功后, 对数据进行操作
5. 操作完毕, 提交事务, 释放锁资源

###### 乐观锁

乐观锁认为数据操作处于一种无竞争或者少量竞争的情况, 所以在乐观锁策略不使用锁来处理并发, 而是利用 **系统版本号** 或者 **时间戳** 来控制并发, 它首先获取数据和数据当中的版本控制标识, 然后认为是没有事务与其竞争这个数据资源, 对这个数据资源直接进行处理, 然后在提交事务的时候再对比版本标识, 如果所持有的版本标识在时间维度上低于数据库当中的版本标识就意味着这个事务在处理这个数据的时候有另外的事务已经处理并且提交了对当前数据的处理, 那么这个时候事务就需要放弃操作并且回滚

一般用 **系统版本号** 或者 **时间戳** 来实现

数据安全低、并发效率高

###### 乐观锁的处理流程

1. 线程读取记录和版本号
2. 更新记录
3. 对比当前数据库当中的该记录的版本号和已读取的版本号
4. 如果两个版本号在时间维度上是一致的, 则提交更新并且版本号 + 1, 否则回滚

###### 悲观锁与乐观锁

**悲观并发控制** 和 **乐观并发控制** 又分别叫做 **悲观锁** 和 **乐观锁** 所以乐观锁和悲观锁不是 mysql 当中锁的分类, 而是处理并发的一种 **策略** 或者一种 **态度**

#### 多版本控制

##### 什么是多版本控制

多版本控制是为了提升并发的性能, 在很多情况下避免了加锁的操作, 因此和锁对比起来开销更低, 所以 MVCC 只是在 **读操作** 远多于 **写操作** 的情况下

MVCC 只在 **REPEATETABLE READ** **READ COMMITTED** 两个隔离级别下工作

##### 版本控制的工作方式

在 InnoDB 当中, MVCC 会在每一行记录后面保存两个隐藏的列, 一个保存了 **行的创建时间**, 一个保存了 **行的过期时间**, 这两个时间是系统版本号, 每一次事务开始的时候会自动递增, 然后把这个版本号当作事务的版本号, 用来和查询得到的每行记录的版本号进行比较, 这样每一次事务开始的时候都得到了这个时间点的数据快照, 无论事务需要运行多长的时间都可以使得它的数据是一样的

MVCC 的具体工作方式要分四种 **SELECT** **INSERT** **DELETE** **UPDATE**

###### SELECT

**SELECT** 的操作通常会返回一个数据集, 所以 MVCC 模式下的 SELECT 会以下面两个要求来塞选数据

- InnoDB 只查找比当前事务把版本早的系统版本, 也就是说行的系统版本要小于或者等于当前事务版本, 这样可以确保得到的每一行数据, 要么是事务开始前就存在的, 要么是事务自己插入或者修改的
- **行的过期时间** 标记要么没有定(为空) 要么大于当前的事务版本号(这样就表示事务读取到这一行的时候, 这个数据是没有被删除的)

###### INSERT

每插入一行新的数据就在这行数据的 **行创建时间** 当中填入当前事务的版本号

###### DELETE

删除一行数据的时候就更新这行数据的 **行过期时间** 为当前事务的版本号

###### UPDATE

将插入一行新的数据, 把这行新的数据的 **行创建时间** 填入当前事务版本号, 将原被更新的数据的 **行过期时间** 更新为当前事务版本号

## 查询

#### 查询执行过程

1. 客户端发送查询到服务器
2. 服务器检查缓存是否命中, 若命中则直接返回
3. 服务器解析 SQL, 并对 SQL 进行预处理
4. 优化器根据处理好的 SQL 生成执行计划
5. Mysql 根据执行计划, 调用存储引擎的 API 进行查询
6. 把查询结果返回客户端

*!* 进行大查询的时候, 在调用存储引擎的 API 时, 查询集合会占用大量的内存, 直到查询执行完毕才会释放内存

#### 查询状态

每一个链接都有它自己的状态, 该状态表示了 mysql 当前正在做什么

- Sleep 等待客户端发送新的请求
- Query 执行查询或者将结果返回给客户端
- Lokced 等待获取锁权限
- Analyzing and statistics 收集存储引擎的统计信息
- Copying to map table 正在执行查询, 并且将结果集复制到一个临时表当中
- Sorting result 对结果集进行排序
- Sending data 在多个状态之间传输数据 || 生成结果集 || 向客户端返回数据

#### 查询缓存

查询进入到服务器的时候, 如果服务器的查询缓存是被打开的, 那么 MySQL 会先看这次查询是否先命中了缓存, 命中检查是通过一个大小写敏感的哈希操作执行的, 所以就算有一个字符匹配不上也不会命中, 如果命中缓存, 还需要在这里检查一下用户的权限. 如果都没有问题则返回数据

#### 语法解析器

语法解析器会使用 MySQL 的愈发规则验证和解析查询, 最后根据 SQL 语句生成一颗解析树, 例如验证关键字是否正确, 关键字顺序是否正确

#### 预处理器

预处理器根据得到的解析树再更具 Mysql 的谷子额进一步检查解析树是否正确, 例如: 检查数据表和数据列是否存在等

#### 查询优化器

一个合法的语法解析树有很多中执行的方式, 查询解析器根据自身的模型选择一条最合适的执行方式去执行

优化策略的参考因素: **每个表或者索引的页面个数** **索引的基数** **索引和数据行的长度** **索引的分布情况**

##### 优化策略

静态优化和动态优化

- 静态优化 类似于“静态编译”, 在整个查询当中执行一次, 静态优化执行一次后就一直存在, 例如在 `where` 条件当中通过一些简单的代数变换换成另外的一种等价方式

- ##### 动态优化 类似于“动态编译”, 每次查询的时候都需要根据上下文的信息和别的因素来动态调节

##### 优化类型

- 重新定义关联表的顺序
- 外链接转化成一个内链接, 只是将等价于内连接的外连接操作转化成内连接
- 等价变换规则简化表达式, 例如 (5 = 5 AND a > 5) 优化成 a > 5
- 优化 `COUNT(*)` 一些索引当中会记录表的行数, 所以这个函数相当于一个常数
- MIN() MAX() 优化 在基于 B-Tree 数据结构的索引上, MIN() 相当于 B-Tree 最左边的数, MAX() 相当于最右边的数
- 覆盖索引扫描 如果索引当中有数据行, 那么就可以直接获取到数据
- 子查询优化 MYSQL 可以在特定情况下, 将子查询转化成一种效率更高的形式

#### 关联查询如何执行

MySQL 对任何关联都执行潜逃循环关联操作, 现在一个表当中取出单条记录, 然后再嵌套循环到下一个表当中寻找匹配的行, 一次下去, 直到所有的表当中匹配的行为止, 再最后一个表当中, 找到所有匹配行, 如果最后一个表也没有了, 再返回再找是否有更多的匹配记录。

如果要执行四个表 **tb1** **tb2** **tb3** **tb4** 的关联操作, Mysql 会从一个表开始一直嵌套循环、回溯完成所有的操作, 所以可能就会是

```
              join
           ---------
           |       |
          join    tb4
       ---------
       |       |
      join    tb3
    --------
    |      |
   tb1    tb2
```

#### 关联查询优化器

在各种表的关联顺序当中找到一个代价最小的关联顺序, 然后生成以课最优的执行树

关联优化器会便利每一张表逐个做嵌套循环计算每一颗可能的执行计划树的成本, 当表数量很多(超过 optimizer_search_depth 参数)的时候就会使用贪婪算法来计算

#### 排序优化

除 B-Tree 索引外, 一些索引没有办法排序就需要 mysql 自己排序, mysql 使用两个排序算法

- 两次传输排序 读取出行指针和排序的列中的字段, 排好序后再去数据表当中找到数据, 占用的空间少, 但是由于两次读取表数据, 会慢一点
- 单次传输排序 读取出需要排序的列和查询需要的列, 排好序后直接返回, 占用的空间多, 由于只读表一次所以效率会快一点

#### 返回结果给客户端

MYSQL 将结果集返回给客户端是一个增量、逐步的过程, 当服务器开始生成第一条记录的时候m Mysql 就可以开始向客户端逐步返回结果集了

## 数据库调优

#### 慢查询

在 Mysql 执行任务时, 我们把查询的时间超过指定时间的查询叫做慢查询

- 开启慢查询功能 `show variable like slow%;`
- 查看慢查询的时间定义 `show variable like 'long%';`
- 设置慢查询的时间 `set long_query_time = 0.0001;`
- 保存慢查询到日志 `set global slow_query_log = ON;`

#### 分解关联查询

将联表查询分解成为多个单表查询, 有如下的优势

1. 减少了一次需要锁定的表的数量, 从而减少了锁竞争
2. 应用层做组合, 可以达到某些记录只查询一次, 这样减少了冗余
3. 提高缓存的应用效率

#### 切分查询

由于 Mysql 的链接与断开链接都很轻量级, 所以如果在通讯允许的情况下, 是可以将一个大查询切成多个小查询, 但也需要考虑实际情况

把一个大数量级的查询切分成为多个小数量级的查询

例如: 服务器定时清理数据, 假设每次清理需要清理100万条数据, 那么一次性的运行这个查询, 就需要锁住很多数据、占满整个事务日志、耗尽系统的资源等等, 但是如果切换成多个小数量级的查询, 就会使得服务器能够有足够的精力去响应别的查询

#### 是否查询了不必要的数据

1. 由于查询的语句没有做限制例如 `limit` 会取出多余的没有必要的数据行
2. 由于错误的使用了 `select *` 导致取出了多余的没有必要的数据列

#### 是否扫描了全表

可以建立合适的索引来避免

#### 经验

1. 首先考虑在 `where` `order by` 的列上建立索引
2. 尽量避免使用 `or` 否则索引会失效, 例如 `select number from t where id = 10 or id = 20` 因改为 `select number from t where id = 10 UNION all select number from t where id = 20`
3. 不要在 `where` 子语句当中使用表达式与函数操作, 这样无法使用索引, 例如 `select number from t where id/2 = 100` 因改为 `select number from t where id = 50`
4. 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引
5. 在使用索引字段作为条件时, 如果该索引是复合索引, 那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引, 否则该索引将不会被使用, 并且应尽可能的让字段顺序与索引顺序相一致
6. 并不是所有索引对查询都有效, SQL是根据表中数据来进行查询优化的, 当索引列有大量数据重复时, SQL查询可能不会去利用索引, 如一表中有字段 **sex** **male** **female** 几乎各一半, 那么即使在sex上建了索引也对查询效率起不了作用
7. 索引并不是越多越好, 索引固然可以提高相应的 select 的效率, 但同时也降低了 insert 及 update 的效率, 因为 insert 或 update 时有可能会重建索引
8. 应尽可能的避免更新 clustered 索引数据列, 因为 clustered 索引数据列的顺序就是表记录的物理存储顺序, 一旦该列值改变将导致整个表记录的顺序的调整, 会耗费相当大的资源
9. 尽可能的使用 varchar/nvarchar 代替 char/nchar, 因为首先变长字段存储空间小, 可以节省存储空间, 其次对于查询来说, 在一个相对较小的字段内搜索效率显然要高些
10. 任何地方都不要使用 select * from t, 用具体的字段列表代替 `*`, 不要返回用不到的任何字段
11. 尽量避免大事务操作, 提高系统并发能力

## 分库分表

#### 什么是分表

如果有两张 user_0 user_1 表，按照它的自增长 id 数列来分表，`id % 2 == 0`，则在 user_0 表当中，如果 `id % 2 == 1` 则在 user_1 表当中

查询时，user_#{number}，其中 number 要为可变参数，而且不能有 “”，参加 $ 与 # 的区别

#### 分表的种类

##### 水平拆分

也就是把表横向切一刀，数据表的结构还是不变，但是数据量变了，这多出来的这些数据量塞到另外的表上去

##### 垂直拆分

也就是把表纵向划一刀，数据表的结构变得更细来了，把原来与该表无关的数据信息或者结构都切换到新的表上去，这样全表扫描的时候，所需要的时间更少

#### 如何分表

##### 使用工具

例如 mycat

第一步在 conf 文件当中配置分库信息

然后修改负载的方式，在客户端直接链接 mycat 就能转发请求

url:http://blog.csdn.net/leisure_life/article/details/78616106

##### 自定义策略

例如提前把一张表分作好几张，然后再根据特定的策略把查询、更新语句打到特定的表上去执行

这样的方法有 hash、id值区间划分等等方法

#### 为什么要分表

在应用程序运行的时候，会有一些数据表会被频繁的插入与修改，就会使得数据表的大小增长很快，这个时候如果当数据的量到一定的程度的时候，会有存储空间的限制而且查询和更新的操作就会变得很慢，这个时候就会考虑把一张数据库表分成多张数据库表或者是多个数据库来存放原本的数据

1. 解决磁盘系统的最大限度

FAT16(最大分区2GB,最大文件2GB ,最大容16G)
FAT32(最大分区32GB，最大容量2TB，最大文件32G)
NTFS(最大分区2TB，最大容量，最大文件2TB)
ext3(最大文件大小: 2TB，最大文件极限: 仅受文件系统大小限制，最大分区/文件系统大小: 4TB，最大文件名长度: 255 字符)

2. 减少锁的竞争

如果把数据分到不同的表当中去，当一个事务正在更新的时候，如果获取的是一个表锁，那么插入操作很有可能不是与更新操作位于同一张表上面，就可以减少锁的竞争的产生

3. 减少 sql 查询的时间

分库分表可减少一张表的数据量，这样就可以减少一条 sql 运行的时间，就可以使得别的 sql 等待时间减少

## 基本概念

#### 内联接

内联接使用比较运算符根据每个表共有的列的值匹配两个表中的行。例如，检索 students和courses表中学生标识号相同的所有行

#### 外联接

外联接可以是左向外联接、右向外联接或完整外部联接。   

1）LEFT JOIN 或 LEFT OUTER JOIN     

左向外联接的结果集包括 LEFT OUTER 子句中指定的左表的所有行，而不仅仅是联接列所匹配的行。如果左表的某行在右表中没有匹配行，则在相关联的结果集行中右表的所有选择列表列均为空值。       

2）RIGHT JOIN 或 RIGHT  OUTER  JOIN     

右向外联接是左向外联接的反向联接。将返回右表的所有行。如果右表的某行在左表中没有匹配行，则将为左表返回空值。       

3）FULL JOIN 或 FULL OUTER JOIN

完整外部联接返回左表和右表中的所有行。当某行在另一个表中没有匹配行时，则另一个表的选择列表列包含空值。如果表之间有匹配行，则整个结果集行包含基表的数据值。

#### 子查询

子查询就是查询中又嵌套的查询,嵌套的级数随各数据库厂商的设定而有所不同,一般最大嵌套数不超过15级,实际应用中,一般不要超过2级,否则代码难以理解.

最普遍的作用是当作上层查询的限制条件，比如select a from b where a in (select a from c where x=y) ，现实世界各种系统中，各式各样的关系，这样子查询从别处提取数据作为另一个查询的条件，这样子查询的作用就显示出来了

#### 关联查询

关联查询使用两个表的相关联的键为依据，合并两个表进行查询

比较常见的比如某工厂　生产X表　里面有 a　日期 b　订单　两字段 ,　销售Y表　b 订单　d　购买客户　,现在我们想看到某天是哪些客户买的东西　然后连接 x y表得到 a b d 这样　整个数据明细就出来了　
